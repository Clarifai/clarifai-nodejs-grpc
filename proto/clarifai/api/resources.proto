syntax = "proto3";

import "proto/clarifai/api/status/status.proto";
import "proto/clarifai/api/utils/extensions.proto";
import "proto/clarifai/auth/util/extension.proto";

import "google/protobuf/struct.proto";
import "google/protobuf/timestamp.proto";

package clarifai.api;

option go_package = "github.com/Clarifai/clarifai-go-grpc/proto/clarifai/api/api";
option java_multiple_files = true;
option java_package = "com.clarifai.grpc.api";
option objc_class_prefix = "CAIP";






////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/annotation.proto
////////////////////////////////////////////////////////////////////////////////
message Annotation {
  reserved 4, 5, 6, 11, 12;

  // The ID for the annotation
  string id = 1;

  // ID of the input this annotation is tied to
  string input_id = 2;

  // The data passed along in this annotation.
  Data data = 3;

  // task_id is deprecated in annotation_info. Use task_id
  google.protobuf.Struct annotation_info = 13;

  // ID of the user this annotation is created by
  string user_id = 15;
  // ID of the model version this annotation is created by
  string model_version_id = 16;

  // DEPRECATED.
  string embed_model_version_id = 14 [deprecated=true];

  // Annotation Status
  clarifai.api.status.Status status = 7;

  // When the annotation was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 8;

  // When the annotation was modified.
  google.protobuf.Timestamp modified_at = 9;

  // Whether or not this annotation is trusted
  // Will be deprecated
  bool trusted = 10[deprecated=true];

  // Is this the input level annotation.
  bool input_level = 17;

  // Consensus review related information, e.g.
  // * annotation group
  // * id of annotation parent, in case the annotation was split from another annotation
  google.protobuf.Struct consensus_info = 18;
  // The id of the task annotation belongs to
  string task_id = 19;
}




////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/app.proto
////////////////////////////////////////////////////////////////////////////////
message App {
  reserved 10, 11, 12;
  string id = 1;
  string name = 2;
  string default_language = 3;
  string default_workflow_id = 4;
  //why is user_id present here when this message type is used in PostApps but completely ignored there? PostApp already specifies the userid in path but doesn't even actually use neither of userids, it instead used the id from auth context.
  //This creates a lot of ambiguity, should always have different message types for Post/Get endpoints so that the minimum interface for each op can be described
  string user_id = 5;
  // When the app was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 6;
  // When the app was last modified
  google.protobuf.Timestamp modified_at= 17;
  // if user accept legal consent for face recognition
  uint32 legal_consent_status = 7;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 13;

  // short description about the app.
  string description = 14;

  // Default value for model predictions on video: Sample delay for video predicting (1 frame per N milliseconds)
  uint32 sample_ms = 15;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 16;

  // data tier id this app is using.
  string data_tier_id = 18;

  // Is starred by the requesting user (only showed on get/list requests)
  // Please use PostAppStars/DeleteAppStars endpoints to star/unstar an app
  bool is_starred = 19;
  // How many users have starred the app (only showed on get/list requests)
  // Computed value, not editable
  int32 star_count = 20;
}

message AppQuery {
  // Query by application name. This supports wildcard queries like "gen*" to match "general" as an example.
  string name = 1;
}




////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/app_sharing.proto
////////////////////////////////////////////////////////////////////////////////
message Collaborator {
  //id of this collaborator
  string id = 1;
  //the app this collaborator has access to
  // FIXME(zeiler): this should be in the user_app_id.app_id already from the endpoint.
  clarifai.api.App app = 2;
  //who is this collaborator
  clarifai.api.User user = 3;
  //the permission this collaborator
  repeated string scopes = 4;
  repeated string endpoints = 5;

  // When the app was shared with. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 6;

  // When the collaborator was updated.
  google.protobuf.Timestamp modified_at = 7;

  // When the collaborator was removed from app.
  google.protobuf.Timestamp deleted_at = 8;
}

//collaboration includes an app you're invited to work on.
message Collaboration{
  //the application
  App app = 1;
  //the app owner's info(including user_unique_id, first_name, last_name, primary_email)
  User app_owner = 2;
  //the low-level scope users are shared with for this collaboration
  repeated string scopes = 3;
  //the endpoint-level scopes users are shared with for this collaboration
  repeated string endpoints = 4;
  //when is the collaboration created
  google.protobuf.Timestamp created_at = 5;
}




////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/audio.proto
////////////////////////////////////////////////////////////////////////////////
message Audio {
  // This is a URL to a publicly accessible image file. The platform will download this file server
  // side and then process.
  string url = 1;
  // The base64 field is using image file bytes directly in the request.
  // NOTE: if you're sending a json request, then this MUST be base64 encoded before sending (hence
  // the name here).
  // When using our grpc clients, you DO NOT need to base64 encode
  // it yourself since the clients know how to do this for you automatically and will avoid the
  // base64 encoding if they send a binary request.
  bytes base64 = 2;
  // If True then you will be allowed to have multiple urls.
  bool allow_duplicate_url = 4;
  // The hosted field lists original audio hosted in Clarifai storage. This field is currently used
  // only in response.
  HostedURL hosted = 5;
}

// Track proto encodes information of a track over a number of frames
message Track {
  // track id
  string id = 1;

  // This is a recursive definition which can contain all the concepts,
  // embeddings, etc. that are computed within this track.
  Data data = 2;

  // Here is where we store the track metadata.
  message TrackInfo {
    // Number of frames
    uint32 num_frames = 1;
    // Timestamp where track begins.
    google.protobuf.Timestamp begin_time = 2;
    // Timestamp where track ends.
    google.protobuf.Timestamp end_time = 3;
    // Quality of the track.
    float quality = 4;
  }

  // The track information.
  TrackInfo track_info = 3;

  TimeInfo time_info = 4;
}










////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/cluster.proto
////////////////////////////////////////////////////////////////////////////////
message Cluster {
  string id = 1;

  // Number of annotations tied to the cluster in the app
  uint32 count = 2;

  // The score assigned to this cluster
  float score = 3;

  // Representative hits for cluster (for now we only return 1)
  repeated Hit hits = 4;

  repeated float projection = 5;
}







////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/code.proto
////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/color.proto
////////////////////////////////////////////////////////////////////////////////
message Color {
  string raw_hex = 1;
  W3C w3c = 2;
  float value = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
}

message W3C {
  string hex = 1;
  string name = 2;
}




////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/common.proto
////////////////////////////////////////////////////////////////////////////////
// Common message to identify the app in a url endpoint.
message UserAppIDSet {
  // Note user_id 'me' is reserved - it is the alias for the id of authorized user
  string user_id = 1;
  string app_id = 2;
}

message PatchAction {
  // The operation to perform on the patched metadata given a path
  // For now only operations 'overwrite', 'delete, and 'merge' is supported
  string op = 1;

  // If the action is 'merge' and there is a conflict, how to resolve it.
  // The options are
  // 'overwrite_by_id', 'remove_by_id', 'merge_by_id','overwrite', 'append' and 'do_nothing'
  // Note that for conflict resolutions '*_by_id' to work on a list, the list should contain
  // objects with an 'id' field which will be used to uniquely identify each field. For example
  // Patching existing json
  // {
  //   "tag": [
  //     {
  //       "id": "1",
  //       "data": 1
  //     },
  //     {
  //       "id": "2",
  //       "data": 2
  //     }
  //   ]
  // }
  // with op 'merge' and merge_conflict_resolution 'overwrite_by_id'
  // {
  //   "tag": [
  //     {
  //       "id": "2",
  //       "data": 3
  //     }
  //   ]
  // }
  // would produce
  // {
  //   "tag": [
  //     {
  //       "id": "1",
  //       "data": 1
  //     },
  //     {
  //       "id": "2",
  //       "data": 3
  //     }
  //   ]
  // }
  // while with merge_conflict_resolution 'remove_by_id' it would produce
  // {
  //   "tag": [
  //     {
  //       "id": "1",
  //       "data": 1
  //     }
  //   ]
  // }
  //
  // Option 'append' will simply create a list on conflicts. For example in above example
  // the final result would be
  // {
  //   "tag": [
  //     {
  //       "id": "1",
  //       "data": 1
  //     },
  //     {
  //       "id": "2",
  //       "data": [2, 3]
  //     }
  //   ]
  // }
  string merge_conflict_resolution = 2;

  // Path for the change. For example 'tag[1].data' is a valid path in above example.
  // Default path is root level i.e. ''.
  string path = 3;
}


////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/concept.proto
////////////////////////////////////////////////////////////////////////////////
message Concept {
  // The concept's unique id.
  string id = 1;
  // The name of the concept in the given language.
  string name = 2;
  // Used to indicate presence (1.0) or not (0.0) of this concept when making a request.
  // This is also the prediction probability when returning predictions from our API.
  // For convenience we use the default of 1.0 when making requests so the concept you provide is
  // is treated as a positive (1.0) and not a negative (which would be value == 0.0).
  float value = 3 [(clarifai.api.utils.cl_default_float) = 1.0, (clarifai.api.utils.cl_show_if_empty) = true];
  // When the concept was created. The format is https://www.ietf.org/rfc/rfc3339.txt .
  // Example: "2006-01-02T15:04:05.999999Z". This field is used only in a response.
  google.protobuf.Timestamp created_at = 4;

  // The language in which the concept name is in. This is *ONLY* used in the response and setting
  // it in a request is ignored since the default language of your app is used when creating
  // or patching a Concept. To set other languages for your concept use the ConceptLanguage object
  // and its corresponding endpoints.
  string language = 5;
  // The application id that this concept is within. This can be ignored by most users.
  string app_id = 6;
  // The definition for the concept. Similar to name. This can be ignored by most users.
  string definition = 7;
  // The vocabulary that this concept belongs to. This is useful if you have different unique sets
  // of concepts that you can separate out based on this field. For example "age_appearance" vs
  // "gender_appearance" in a list of concept returned from the demographics model.
  string vocab_id = 8;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 9;

  // The user the concept belongs to.
  string user_id = 10;
}

message ConceptCount {
  // The concept's unique id.
  string id = 1;
  // The name of the concept.
  string name = 2;
  // The total count for concepts labeled for all asset statues (processing, to_process, processed, error)
  ConceptTypeCount concept_type_count = 3;
  // The detail count for different assets status
  DetailConceptCount detail_concept_count = 4;
}

message ConceptTypeCount {
  // The number of inputs that have a concept with a value of 1.0 (indicating presence of the
  // concept in an input).
  uint32 positive = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The number of inputs that have a concept with a value of 0.0 (indicating absence of the
  // concept in an input).
  uint32 negative = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
}

message DetailConceptCount {
  // The concept count for processed assets
  ConceptTypeCount processed = 1;
  // The concept count for to process assets
  ConceptTypeCount to_process = 2;
  // The concept count for assets with status error
  ConceptTypeCount errors = 3;
  // The concept count for processing assets
  ConceptTypeCount processing = 4;
}

message ConceptQuery {
  // The name of the concept to search.
  string name = 1;
  // (optional) The language of the concept name in a search. Defaults to English.
  string language = 2;
  // (optional) The id of workflow. If no id is provided, then application base workflow is used.
  string workflow_id = 3;
}




////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/concept_graph.proto
////////////////////////////////////////////////////////////////////////////////
// This represents a relation (i.e. edge) between the subject concept and the object concept
message ConceptRelation {
  // ID of the concept relation
  string id = 1;

  // The subject concept (i.e. source) of the concept relation
  Concept subject_concept = 2;

  // The subject concept (i.e. destination) of the concept relation
  Concept object_concept = 3;
  // The predicate (i.e. edge) linking the subject and the object
  // Both subject_concept and object_concept are concepts.
  // The predicate is the type of relationship.
  // That predicate acts on the subject.
  //
  // There are three current types of predicates:
  // 1) "hyponym"
  // 2) "hypernym"
  // 3) "synonym"
  //
  // 1) For example, 'hyponym' is a type of predicate which represents 'is_a_kind_of' relation so
  // the following relationship:
  // 'honey' (subject), 'hyponym' (predicate), 'food' (object)
  // Can more easily be read as:
  // 'honey' 'is a kind of' 'food'
  //
  //
  // 2) The 'hypernym' relation is the opposite of 'hyponym' and when you add one of the
  // relationships the opposite will automatically appear for you in queries.
  //
  // The 'hypernym' can be read as 'is a parent of' so:
  // 'food' (subject), 'hypernym' (predicate), 'honey' (object)
  // Can more easily be read as:
  // 'food' is a parent of 'honey'
  //
  // 3) The 'synonym' relation defines two concepts that essential mean the same thing. This
  // is more like a "is" relationship. So for example a 'synonym' relationship could be:
  // "puppy" is "pup"
  // The reverse is also true once the former is added so:
  // "pup" is "puppy"
  // will appear in queries as well.
  string predicate = 4;

  // The knowledge graph id that this edge belongs to. If using the app's global knowledge graph
  // and not a specific one then this should be the empty string "".
  string knowledge_graph_id = 5;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 6;
}

// A Knowledge Graph is a logical subsets of edges in the overall Concept Graph
message KnowledgeGraph {
  // ID of the knowledge graph
  string id = 1;
  // Name of the knowledge graph
  string name = 2;
  // Human readable description of the knowledge graph
  string description = 3;
  // The app that contains the images that correspond to the concepts in the knowledge graph
  string examples_app_id = 4;
  // The app that contains the sample images that we want to show the customer for the concepts in the knowledge graph
  string sampled_examples_app_id = 5;
}


message ConceptMappingJob {
  // The id of the knowledge graph being used for this concept mapping job
  string knowledge_graph_id = 1;
  // The ids of the concepts being mapped
  repeated string concept_ids = 2;
}




////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/concept_language.proto
////////////////////////////////////////////////////////////////////////////////
// This represents a link to an outside source for the given concept.
// The values from here are sticked into Concept message into the name and definition fields when
// returning from the API in your default language. The "id" field here becomes the "language"
// field of the Concept message which is a little weird.
message ConceptLanguage {
  // This is the language code for the language such as "en".
  string id = 1;
  // The type of the outside source.
  string name = 2;
  // The ID that is referenced in the source.
  string definition = 3;
}








////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/data.proto
////////////////////////////////////////////////////////////////////////////////
message Data {
  reserved 4, 10;
  // Input and output images.
  Image image = 1;
  // Input and output videos.
  Video video = 2;
  // A list of concepts.
  repeated Concept concepts = 3;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 5;
  // Geography information.
  Geo geo = 6;

  // The dominant colors within an image.
  repeated Color colors = 7;
  // Clustering centroids for inputs.
  repeated Cluster clusters = 8;
  // Embedding vectors representing each input.
  repeated Embedding embeddings = 9;
  // For recursing into localized regions of an input.
  repeated Region regions = 11;
  // For temporal content like video.
  repeated Frame frames = 12;
  // Input and output text.
  Text text = 13;
  // Input and output audio.
  Audio audio = 14;
  // Track information.
  repeated Track tracks = 15;
  // Time segments information.
  repeated TimeSegment time_segments = 16;
}

// A region within the data.
message Region {
  // A unique id for the region.
  string id = 1;
  // The details about the location of the region.
  RegionInfo region_info = 2;
  // A recursive definition of the data within the Region. For example, this will contain
  // data.concepts if the region also has annotations or predictions of concepts within it.
  Data data = 3;
  // This is the confidence score of the overall Region.
  float value = 4;
  // For tracking algorithsm and annotations we tie regions together with this track id.
  string track_id = 5;
}

// The information of the location of the Region.
message RegionInfo {
  reserved 2, 3;

  // Details of the region's rectangular bounding box.
  BoundingBox bounding_box = 1;
  // Details of the region's segmentation mask.
  Mask mask = 4;
  // A polygon of points.
  Polygon polygon = 5;
  // A landmark point location.
  Point point = 6;
}

// Rectangular bounding box for a region.
message BoundingBox {
  // The top left of the bounding box normalized to the data dimension to be within [0-1.0]
  float top_row = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The left column of the bounding box normalized to the data dimension to be within [0-1.0]
  float left_col = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The bottom row of the bounding box normalized to the data dimension to be within [0-1.0]
  float bottom_row = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The right col of the bounding box normalized to the data dimension to be within [0-1.0]
  float right_col = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
}

// The information of the location of the Frame.
message FrameInfo {
  // The index of the frame. Keep in mind that this depends on the sampling rate used during
  // processing.
  uint32 index = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  // time in the video in milliseconds. This is independent of the sampling rates used during
  // processing.
  uint32 time = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
}

// A Frame of time-series Data such as a Video.
message Frame {
  // Information aboue frame such as number and time.
  FrameInfo frame_info = 1;
  // A recursive definition of the data within the Frame. For example, this will contain
  // data.concepts if the Frame also has annotations or predictions of concepts within it.
  // This can also have data.regions for annotation or predictions of detection regions, which can
  // then recursively have their data field filled in as well.
  Data data = 2;
  // An ID for the frame.
  string id = 3;
}

// Segmentation mask.
message Mask {
  reserved 1;

  // The image of the mask in a non-raster format.
  Image image = 2;
}

message Polygon {
  // A list of points connected together to form the polygon.
  repeated Point points = 1;
}

message Point {
  // The row location of the point. This has a [0.0-1.0] range with 0.0 being top row and 1.0
  // being the bottom row.
  float row = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The column location of the point. This has a [0.0-1.0] range with 0.0 being left col and 1.0
  // being the right col.
  float col = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Depth if applicable for the point.
  float z = 3;
}




////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/embedding.proto
////////////////////////////////////////////////////////////////////////////////
message Embedding {
  repeated float vector = 1 [packed = true];
  uint32 num_dimensions = 2;
}




////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/geo.proto
////////////////////////////////////////////////////////////////////////////////
message GeoPoint {
  float longitude = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  float latitude = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
}

message GeoLimit {
  string type = 1;
  float value = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
}

message GeoBoxedPoint {
  GeoPoint geo_point = 1;
}

message Geo {
  GeoPoint geo_point = 1;
  GeoLimit geo_limit = 2;
  // NOTE: inconsistency: should have been geo_boxed_points
  repeated GeoBoxedPoint geo_box = 3;
}







////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/healthz.proto
////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/image.proto
////////////////////////////////////////////////////////////////////////////////
message Image {
  reserved 3;

  // This is a URL to a publicly accessible image file. The platform will download this file server
  // side and then process.
  string url = 1;
  // The base64 field is using image file bytes directly in the request.
  // NOTE: if you're sending a json request, then this MUST be base64 encoded before sending (hence
  // the name here).
  // When using our grpc clients, you DO NOT need to base64 encode
  // it yourself since the clients know how to do this for you automatically and will avoid the
  // base64 encoding if they send a binary request.
  bytes base64 = 2;

  bool allow_duplicate_url = 4;
  // The hosted field lists images in different sizes hosted in Clarifai storage.
  HostedURL hosted = 5;
}

message HostedURL {
  // Prefix of the URL of every hosted image.
  string prefix = 1;
  // Suffix of an image stored in different sizes.
  string suffix = 2;
  // The sizes field lists which images of the different sizes are hosted in our storage. The URL
  // of each hosted image can be obtained by joining the prefix, one of the sizes and suffix.
  repeated string sizes = 3;
  // The crossorigin property of html media tag
  // For Secure Data Hosting this needs to be set to 'use-credentials'
  string crossorigin = 4;
}




////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/input.proto
////////////////////////////////////////////////////////////////////////////////
message Input {
  reserved 3;

  // The ID for the input
  string id = 1;

  // The data passed along in this input.
  Data data = 2;

  // When the input was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 4;

  // When the input was modified.
  google.protobuf.Timestamp modified_at = 5;

  // This is the status at a per Input level which allows for
  // partial failures.
  clarifai.api.status.Status status = 6;

  // List of dataset IDs that this input is part of
  // Currently, this field is ONLY used in search.
  repeated string dataset_ids = 7;
}

// NOTE: inconsistency: this is weird mix of plural and singular words.
message InputCount {
  uint32 processed = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 to_process = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 errors = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 processing = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 reindexed = 5 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 to_reindex = 6 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 reindex_errors = 7 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 reindexing = 8 [(clarifai.api.utils.cl_show_if_empty) = true];
}









message WorkflowResultsSimilarity {
  // The input with the specific data compare against all pool results
  Input probe_input = 1;
  repeated Hit pool_results = 2;
}

////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/key.proto
////////////////////////////////////////////////////////////////////////////////
message Key {
  // The id of this key, it is used for authorization.
  string id = 1;
  // The type of key, it can be api_key or personal_access_token, the default value is api_key
  string type = 8;
  // The description
  string description = 2;
  // The low-level scopes this key has
  repeated string scopes = 3;
  // The endpoint-level scopes this key has
  repeated string endpoints = 7;
  // The apps that this key give you access to, it is empty if this key is personal_access_token
  // API key can only give you access to a single app.
  repeated App apps = 4;

  // When the key was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 5;

  // When does the key expires, the key won't expire if this is empty
  google.protobuf.Timestamp expires_at = 6;

  // list of idp ids at which key is currently authorized
  repeated string authorized_idp_ids = 9;
}





enum ExpirationAction {
  EXPIRATION_ACTION_NOT_SET = 0;

  DELAY = 1; // Progressively delay the execution of operations
  EXPIRY = 2; // Cease functioning
}

enum LicenseScope {
  LICENSE_SCOPE_NOT_SET = 0;

  PREDICT = 1;
  TRAIN = 2;
  SEARCH = 3;
}





////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/model.proto
////////////////////////////////////////////////////////////////////////////////
// This is the Model object which represents a created model in the platform.
// Each model has a particular type denoted by the model_type_id.
// When creating a Model with PostModels the following happens:
//  - if the ModelType is trainable, then a new ModelVersion is created that is
//    - UNTRAINED status by default
//    - TRAINED status if a ModelVersion was included with PretrainedModelConfig in PostModels
//  - if the ModelType is not trainable, then a new ModelVersion is created with TRAINED status.
// To modify config settings like OutputInfo for the Model you an use PatchModels. This will
// also create a new ModelVersion, potentially UNTRAINED following the same rules as above.
// The fields that are patchable include Model.name, Model.display_name and Model.output_info
// (except the Model.output_info.type and Model.output_info.type_ext).
//
//
message Model {
  reserved 8, 10,11;

  // The model's ID. Must be unique within a particular app and URL-friendly.
  string id = 1;
  // A nicer-to-read name for the model. Can have spaces and special characters.
  string name = 2;
  // When the model was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  //  the following from the API:
  //  "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 3;
  // When was the most recent model version created at
  google.protobuf.Timestamp modified_at = 19;
  // The app the model belongs to.
  string app_id = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Info about the model's output and configuration.
  OutputInfo output_info = 5;
  // A particular version of the model, e.g., to specify the version when creating a workflow.
  ModelVersion model_version = 6;
  // An even nicer-to-read name for public Clarifai models where we're not happy with the name but
  // need a temporary workaround while we check what depends on these names.
  string display_name = 7;
  // The user id that the model belongs to.
  string user_id = 9;
  // Info about the models' input and configuration of them.
  InputInfo input_info = 12;
  // Configuration for the training process of this model.
  TrainInfo train_info = 13;
  // The ModelType.Id that is used for this model. This is used for all versions and you cannot
  // change model_type_id between versions of the same model.
  string model_type_id = 14;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 15;

  // Short description about this model
  string description = 16;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 17;

  // Notes about a model (should support markdown)
  // This field should be used for in-depth notes about
  // about a model and supports up to 64Kbs.
  string notes = 18;
  // Tags from toolkits category
  repeated string toolkits = 20 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Tags from use_cases category
  repeated string use_cases = 21 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Is starred by the requesting user (only showed on get/list requests)
  // Please use PostModelStars/DeleteModelStars endpoints to star/unstar a model
  bool is_starred = 22;
  // How many users have starred the model (only showed on get/list requests)
  // Computed value, not editable
  int32 star_count = 23;
}

// A link to a html/markdown/text file that stores reference material
// tied to a model.
message ModelReference {
  // Id of the reference
  string id = 1;

  // The id of the model this Model reference is tied to.
  string model_id = 2;

  // address of resource
  string url = 3;

  // name of link
  string name = 4;

  // To handle arbitrary json metadata:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 5;
}

message ModelVersionInputExample {
  // user unique id
  string id = 1;
  // external id of model
  string model_id = 2;
  // external id of model version
  string model_version_id = 3;
  // data to store as example input for model
  Data data = 4;
  // name of link for display
  string name = 5;
  // description of link contents
  string description = 6;
}

// OutputInfo defines some of the settings for each model version that PatchModels can effect. These
// parameters control some of the training or inference operations that this model can do.
// As the number of parameters continued to grow when we launched more ModelTypes we decided to move
// to using the OutputInfo.params field which is a Struct (or JSON object if you're using
// our JSON REST APIs). This allows each ModelType to define the set of fields, their default values
// and description of each field so that we can display those in Portal and make the creation of
// Model's very extensible. The OutputConfig object will eventually go away in favor of
// infer_params struct.
message OutputInfo {
  // List of concepts or other output related data for the model.
  Data data = 1;
  // Model configuration...going away in favor of infer_params and train_params over time.
  // TO BE DEPRECATED
  OutputConfig output_config = 2;
  // For returning where to look for the Output info if not returning it.
  string message = 3;
  // To help clients know what type of Data to expect out of the model.
  // TO BE DEPRECATED
  string type = 4;
  // Extra metadata about the Type data.
  // TO BE DEPRECATED
  string type_ext = 5;

  // Map from the api.Data field names to the underlying model graph's outputs. When using a
  // PretrainedModelConfig the values in this map need to match the Triton config.pbtxt output names.
  google.protobuf.Struct fields_map = 6;

  // For predicting with the various ModelType's we accept a Struct (JSON object) worth of args
  // that the ModelTypeField defines. During inference, the settings contained within are sent
  // to the model predictor to alter predictions from this Model.
  google.protobuf.Struct params = 7;
}



message InputInfo {
  // Map from the api.Data field names to the underlying model graph's inputs. When using a
  // PretrainedModelConfig the values in this map need to match the Triton config.pbtxt input names.
  google.protobuf.Struct fields_map = 1;

  // To control the inputs to the given model we allow a list of parameters
  // defined for each ModelType as a Struct (JSON object) here. During training or inference, the
  // settings contained within are sent to the training processor to alter the training process.
  google.protobuf.Struct params = 2;
}

message TrainInfo {
  // To control the training process when PostModelVersions is used we allow a list of parameters
  // defined for each ModelType as a Struct (JSON object) here. During training, the settings
  // contained within are sent to the training processor to alter the training process.
  google.protobuf.Struct params = 1;
}



// OutputConfig is a collection of parameters controlling either inference or training settings for
// the given Model. This message will be deprecated over time in favor or infer_params and
// train_params in OutputInfo which are cleaner and more extensible for many ModelTypes.
message OutputConfig {
  reserved 11, 12, 16, 18;

  // For custom concept model training: whether the concept predictions must sum to 1.
  bool concepts_mutually_exclusive = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  // For custom concept model training: Whether negatives should only be sampled from within the app during
  // training, for custom models.
  bool closed_environment = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  // DEPRECATED: For custom models, this is the base model to use for image embeddings.
  // Default is general model.
  string existing_model_id = 3 [deprecated = true];
  // For concept model predictions: Overrides the default_language for the app in a predict call.
  string language = 4;
  // DEPRECATED: Hyper-parameters for custom training.
  // Use new hyper_params field instead.
  string hyper_parameters = 5 [deprecated = true];
  // For concept model predictions:  Maximum number of concepts in result. Defaults to 0 which under
  // the hood will return default of 20. We do a server side default in order to control this
  // feature in the future.
  uint32 max_concepts = 6 [(clarifai.api.utils.cl_show_if_empty) = true];
  // For concept model predictions: Minimum value of concept's probability score in result.
  // Defaults to 0.0 which means we won't do any thresholding as all probabilities will
  // likely be > 0.0.
  float min_value = 7 [(clarifai.api.utils.cl_show_if_empty) = true];
  // For concept model predictions: Select concepts in result by name or by id
  repeated Concept select_concepts = 8;
  // For custom concept model training: Training timeout of the model (in seconds)
  uint32 training_timeout = 9;
  // For model predictions on video: Sample delay for video predicting (1 frame per N milliseconds)
  uint32 sample_ms = 10;
  // For custom model training: Hyperparameters for custom training
  google.protobuf.Struct hyper_params = 13;
  // For custom model training: this is the base model version to use for image embeddings.
  // This has to be one of the embed models in the app workflow.
  string embed_model_version_id = 14;
  // For custom model training: Use this flag to fail on missing positive examples
  // By default we fill in the missing with random examples
  bool fail_on_missing_positive_examples = 15;
  // For custom model training: This is any additional metadata as a JSON object that we want
  // want to persist in the model's output config. This is a useful quick way to set fields for
  // introducing fields for new model types so we don't have to add a new proto field and DB field
  // each time. Please refer to the documentation or model implementation internally for more
  // details on what fields are supported for which models.
  // TODO(zeiler): remove this field after Portal is updated.
  google.protobuf.Struct model_metadata = 17 [deprecated=true];
}



// ModelSpec is a definition of a Model type. This is used in model mode of portal
// to list out the possible models that can be created and can be used to understand more about
// the possible models in our platform.
message ModelType {
  reserved 7;
  // A unique identifies for this model type. This is differnt than the 'type' field below because
  // the 'type' can be re-used for differnet input and output combinations whereas 'id' is always
  // unique.
  string id = 1;
  // title for this model in model gallery
  string title = 2;
  // Description of this model type.
  string description = 3;
  // The type of the model to create. This is currently stored in output_info.type.
  string type = 4;
  // The list of input fields that this model accepts. These are the keys of the Model's
  // InputInfo.fields_map
  repeated string input_fields = 5;
  // The list of output fields that this model accepts. These are the keys of the Model's
  // OutputInfo.fields_map
  repeated string output_fields = 6;
  // Is this model trainable in our platform.
  bool trainable = 8;
  // Is this model creatable. We have some pre-trained model types that users cannot create yet in
  // model mode.
  bool creatable = 9;
  // Is this model type only for internal users at this time.
  bool internal_only = 10;

  // The remaining fields are definitions of the configurable fields that exist.
  // Each field has path into the Model object such as "name" as a top level or "output_info.data"
  // if it's the Data object within the OutputInfo object. We decided to not break these up
  // into input_info, train_info and output_info related parameters and instead use the path
  // so that they are most flexible.
  repeated ModelTypeField model_type_fields = 11;

  // For sequence models we need to know when processing that they require temporal time frames
  // in sequential order. This will be true for model types like trackers as an example.
  bool requires_sequential_frames = 12;

  // Can this model be evaluated?
  bool evaluable = 13;


  // Maps input_fields to the more granular data fields needed to parse a triton models inputs
  google.protobuf.Struct expected_pretrained_input_fields = 14;

  // Maps output_fields to the more granular data fields needed to parse a triton models outputs
  google.protobuf.Struct expected_pretrained_output_fields = 15;
}

// ModelTypeField stores a field value of a configurable type.
message ModelTypeField {
  // The path where the value of the field will be stored.
  // Example:
  // "output_info.data" would be the Data message in the OutputInfo message.
  // "output_info.output_config.language" is in the OutputConfig message within OutputInfo
  // "input_info.params" is in the params struct within InputInfo.
  // "output_info.params" is in the params struct within OutputInfo.
  // "train_info.params" is in the params struct within TrainInfo.
  // and so on.
  string path = 1;
  // These are various types of fields that we have UIs for.
  enum ModelTypeFieldType {
    reserved 6;

    INVALID_MODEL_TYPE_FIELD_TYPE = 0;

    BOOLEAN = 1;
    STRING = 2;
    NUMBER = 3;
    // For auto-completing to concepts in the app. This goes into an data.concepts field.
    ARRAY_OF_CONCEPTS = 4;
    // For auto-completing to concepts in the app. This goes into an data.concepts field.
    ARRAY_OF_CONCEPTS_WITH_THRESHOLD = 5;
    // A range for a float value.
    RANGE = 7;
    // If ENUM is used then the "enum_options" field should also be filled in with the respective ID and description
    // for the different ENUM options.
    ENUM = 8;
    // For listing collaborators of the app. The field is a string of the collaborator's user_id.
    COLLABORATORS = 9;
    // For arbitrary json object: "{...}"
    JSON = 10;
    // Such as [1.0, 2.0, 3.5]
    ARRAY_OF_NUMBERS = 11;
    // For selecting the embed_model_version_id for context based models.
    // This is a string type in the API request.
    WORKFLOW_EMBED_MODELS = 12;
    // Such as ['a', 'b', 'cantaloupe']
    ARRAY_OF_STRINGS = 13;
    // If RECURSIVE_ENUM is used then the "enum_options" field should also be filled in with the respective ID and description
    // for the different RECURSIVE_ENUM options, as well as model_type_fields for each enum choice.
    RECURSIVE_ENUM = 14;
  }
  // The field for this field.
  ModelTypeFieldType field_type = 2;
  // A default value. We use the Value field because we want to have structured data (just like
  // google.protobuf.Struct but this is just a single value).
  google.protobuf.Value default_value = 3;
  // Description for this field.
  string description = 4;
  // Placeholder text for the UI element.
  string placeholder = 5;
  // List of options of the ENUM type and potentially additional fields they bring with them.
  repeated ModelTypeEnumOption model_type_enum_options = 6;
  // If this field should appear for internal users only.
  bool internal_only = 7;
  // If this field is a required field. If True then during validation you won't be able to create
  // a model of this type with providing a value for this field. When False, the ModelType's
  // default_value will be used for this field.
  bool required = 8;
  // If the field_type is RANGE, this must be filled in.
  ModelTypeRangeInfo model_type_range_info = 9;
}

message ModelTypeRangeInfo {
  // The start of the range as a float.
  float min = 1;
  // The end of the range as a float.
  float max = 2;
  // An optional step size for the range. If provided then only values at that step size will be
  // rounded to. For example if step is 0.02 then 0.0245 will round to 0.02.
  float step = 3;
}

message ModelTypeEnumOption {
  // The unique value of the enum option.
  string id = 1;

  // Optional description for this enum option.
  string description = 2;
  // These are additional fields that are specific to this enum choice. This allows
  // us to use enums to control configuration settings as well.
  repeated ModelTypeField model_type_fields = 3;

  // If this enum option should be internal only.
  bool internal_only = 4;
}

message ModelQuery {
  // The name ofthe field. This supports wilcard queries like "gen*" to match "general" as an example.
  string name = 1;
  // This is the legacy model type. Do not use, only kept for support in old API clients.
  string type = 2 [deprecated = true];
  // Filter models by the specific model_type_id. See ListModelTypes for the list of ModelType.Id's
  // supported.
  string model_type_id = 3;
}
enum ValueComparator {
  CONCEPT_THRESHOLD_NOT_SET = 0;

  // input > value
  GREATER_THAN = 1;
  // input >= value
  GREATER_THAN_OR_EQUAL = 2;
  // input < value
  LESS_THAN = 3;
  // input <= value
  LESS_THAN_OR_EQUAL = 4;
  // input == value
  EQUAL = 5;
}



enum EvaluationType {
  Classification = 0; // default
  Detection = 1;
}




////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/model_version.proto
////////////////////////////////////////////////////////////////////////////////
message ModelVersion {
  reserved 9;

  string id = 1;
  // When the version was created.
  google.protobuf.Timestamp created_at = 2;
  // The status of the version (whether it's untrained, training, trained, etc.).
  clarifai.api.status.Status status = 3;

  uint32 active_concept_count = 4;

  EvalMetrics metrics = 5;

  // number of inputs in the model version
  uint32 total_input_count = 6;

  // This is the internal name for the ModelVersion when creating a new model at Clarifai. If you're
  // an API user you should not need to interact with this field.

  // Detailed training stats.

  // When training of this version was completed.
  google.protobuf.Timestamp completed_at = 10;

  // Description about this version
  string description = 11;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 12;

  // The app the model version belongs to.
  string app_id = 13;
  // The user the model version belongs to.
  string user_id = 14;

  // When this model version was last modified
  google.protobuf.Timestamp modified_at = 15;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 16;

  string license = 17;
}

message PretrainedModelConfig {
  // This is the internal id of the pretrained model.
  string id = 1;
  // This is the internal type of the pretrained model.
  string type = 2;
  // Map from the api.Data field names to the Triton config.pbtxt input.
  google.protobuf.Struct input_fields_map = 3;
  // Map from the api.Data field names to the Triton config.pbtxt output.
  google.protobuf.Struct output_fields_map = 4;
  // Preprocessing steps for the model as a json string
  string data_provider_params = 5;
  // Url to a zipped up model in triton format with only version 1
  string model_zip_url = 6;
  // Whether to overwrite the model for the existing internal id
  bool overwrite = 7;
}

message TrainStats {
  repeated LossCurveEntry loss_curve = 1;
}

message LossCurveEntry {
  // current epoch
  uint32 epoch = 1;
  // current global step
  uint32 global_step = 2;
  // current cost
  // FIXME(rigel): this should be loss instead of cost.
  float cost = 3;
}

message LabelCount {
  // FIXME: should move to Concept object and return the whole thing (including name and id)
  // otherwise if two concepts have same name then you won't tell them apart in confusion matrix.
  string concept_name = 1;
  uint32 count = 2;
}

message LabelDistribution {
  repeated LabelCount positive_label_counts = 1;
}

// NOTE: this is inefficient, should just have the order of the rows/cols
message CooccurrenceMatrixEntry {
  // concept_id for the row
  string row = 1;
  // concept_id for the col
  string col = 2;
  uint32 count = 3;
}

message CooccurrenceMatrix {
  repeated CooccurrenceMatrixEntry matrix = 1;
  // These concept_ids are ordered by the strength of the diagonal in the ConfusionMatrix.
  repeated string concept_ids = 2;
}

message ConfusionMatrixEntry {
  string predicted = 1;
  string actual = 2;
  float value = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
}

message ConfusionMatrix {
  repeated ConfusionMatrixEntry matrix = 1;
  // These concept_ids are ordered by the strength of the diagonal in the ConfusionMatrix.
  repeated string concept_ids = 2;
}

message ROC {
  repeated float fpr = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  repeated float tpr = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  repeated float thresholds = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  repeated float fpr_per_image = 4;
  repeated float fpr_per_object = 5;
}

message PrecisionRecallCurve {
  repeated float recall = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  repeated float precision = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  repeated float thresholds = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
}

message BinaryMetrics {
  uint32 num_pos = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 num_neg = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 num_tot = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  float roc_auc = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
  float f1 = 5 [(clarifai.api.utils.cl_show_if_empty) = true];
  Concept concept = 6;
  ROC roc_curve = 7;
  PrecisionRecallCurve precision_recall_curve = 8;
  float avg_precision = 9;
  string area_name = 10;
  double area_min = 11;
  double area_max = 12;
  float iou = 13;

}

message TrackerMetrics {
  // Multiple object tracking accuracy
  float mot_mota = 1;
  // Number of switches between tracks
  int32 mot_num_switches = 2;
  // MORSE fragmentation rate (a.k.a unique switch rate, only calculated in public sector)
  float morse_frag = 3;
  // Average precision calculated from all processed frames
  float avg_precision = 4;
  // The concept that we are evaluating the tracker
  string aiid = 5;
  // Same as morse_frag but calculated using MOT mapping/metrics
  float unique_switch_rate = 6;
}

message EvalTestSetEntry {
  // Input CFID
  string id = 1 [deprecated = true];
  string url = 2 [deprecated = true];
  Input input = 6; // the input information

  repeated Concept predicted_concepts = 3;
  // All the ground truth concepts will be show on the top level
  repeated Concept ground_truth_concepts = 4;
  // Only region-based/frame-based app contains this annotation
  // Each annotation only contains one region
  // And the concepts is in ground_truth_concepts instead of this annotation
  Annotation annotation = 5;

}

// NOTE(Janvier): We copy this from proto/utils/lopq_service.proto instead of importing it because
// we should not import internal protos in public protos.
message LOPQEvalResult {
  // Rank k for which all metrics are reported.
  int32 k = 1;

  // Recall @ k assuming the brute force search is the ground truth.
  float recall_vs_brute_force = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Kendall's tau correlation @ k assuming the brute force search is the ground truth.
  float kendall_tau_vs_brute_force = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The percentage of the most frequent code in the indexed part of evaluation data.
  float most_frequent_code_percent = 4 [(clarifai.api.utils.cl_show_if_empty) = true];

  // Normalized Discounted Cumulative Gain (NDCG) @ k with a ground truth inferred from annotations
  // and/or prediction for this evaluation LOPQ model.
  // NDCG uses individual relevance scores of each returned image to evaluate the usefulness, or
  // gain, of a document based on its position in the result list. The premise of DCG is that
  // highly relevant documents appearing lower in a search result list should be penalized as the
  // graded relevance value is reduced logarithmically proportional to the position of the result.
  // See: https://en.wikipedia.org/wiki/Information_retrieval#Discounted_cumulative_gain
  //
  // To compute the relevance score between two images we consider two cases:
  // 1) Only one label for each image
  // An image is relevant to an image query iff they are labeled the same (score 1), and
  // not relevant otherwise (score 0)
  // 2) Multiple labels for each image
  // Here an image relevancy with respect to a single image query is measured by f-beta score
  // assuming the query image list of labels as ground truth and comparing them with that of
  // the search result. These labels can come from image annotations or if substitute_annotation_misses
  // is set, predictions of base classifier where any prediction with prob < prob_threshold are
  // discarded. To quantify the relevancy score of a single search result we opt to compute precision
  // and recall @ k for simplicity, and combine them with f-beta score to obtain a single number.
  float lopq_ndcg = 5 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Brute force NDCG which gives a baseline to compare to and is a measure of how good
  // the embeddings are.
  float brute_force_ndcg = 6 [(clarifai.api.utils.cl_show_if_empty) = true];
}

// FIXME: copy this into an internal proto since it is stored in DB and field names can't change.
message MetricsSummary {
  float top1_accuracy = 1 [deprecated = true];
  float top5_accuracy = 2 [deprecated = true];
  float macro_avg_roc_auc = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  float macro_std_roc_auc = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
  float macro_avg_f1_score = 5 [(clarifai.api.utils.cl_show_if_empty) = true];
  float macro_std_f1_score = 6 [(clarifai.api.utils.cl_show_if_empty) = true];
  float macro_avg_precision = 7 [(clarifai.api.utils.cl_show_if_empty) = true];
  float macro_avg_recall = 8 [(clarifai.api.utils.cl_show_if_empty) = true];
  float mean_avg_precision_iou_50 = 10;
  float mean_avg_precision_iou_range = 11;

  repeated LOPQEvalResult lopq_metrics = 9;
}

message EvalMetrics {
  clarifai.api.status.Status status = 1;
  string id = 10;
  MetricsSummary summary = 2;
  ConfusionMatrix confusion_matrix = 3;
  CooccurrenceMatrix cooccurrence_matrix = 4;
  LabelDistribution label_counts = 5;
  repeated BinaryMetrics binary_metrics = 6;
  repeated EvalTestSetEntry test_set = 7;
  repeated BinaryMetrics metrics_by_area = 8;
  repeated BinaryMetrics metrics_by_class = 9;
  repeated TrackerMetrics tracker_metrics = 11;
}



message FieldsValue {
  bool confusion_matrix = 1;
  bool cooccurrence_matrix = 2;
  bool label_counts = 3;
  bool binary_metrics = 4;
  bool test_set = 5;
  bool metrics_by_area = 6;
  bool metrics_by_class = 7;
}


////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/output.proto
////////////////////////////////////////////////////////////////////////////////
message Output {
  // One of these outputs per Input
  string id = 1;
  clarifai.api.status.Status status = 2;

  // When the object was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 3;

  // The model that created this Output.
  Model model = 4;
  // The input that was passed to the model to create this Output. For example if we have an image
  // model then it will take as input here an Input object with Image filled in.
  Input input = 5;
  // The output data for this Output. For example if we have a concept model then the predicted
  // concepts will appear here.
  Data data = 6;
}




////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/scope.proto
////////////////////////////////////////////////////////////////////////////////
message ScopeDeps {
  // The scope
  string scope = 1;
  // Other scopes that are required.
  repeated string depending_scopes = 2;
}

message EndpointDeps {
  // The fully qualified endpoint to
  string endpoint = 1;
  // Other scopes that are required.
  repeated string depending_scopes = 2;
}




////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/search.proto
////////////////////////////////////////////////////////////////////////////////
message Hit {
  // This is the score for the ranked Hit results of the search query. This score is a number
  // between 0.0 and 1.0 as it represents a confidence in the search Hit. For example, if you search
  // for "car" and get a close matching Hit, the score should be close to 1.0. If you get a score
  // of close to 0.0 that means it's very disimilar to your query, in this case NOT a "car". There
  // is a special intermediate score of 0.5 that means that the Hit is not really correlated with
  // your search query (ie. not similar or dissimlar to the query) which is a common occurrence
  // when using negate queries.
  // Note: some queries that are just filtering down your app of inputs may just return a score of
  // 1.0 for all Hits.
  float score = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  // This is the matched input returned from the search query. This will contain information about
  // the Input such as the url, created_at time and trusted annotation information (for backwards
  // compatibility with apps that existed before Annotations were introduced.
  Input input = 2;
  // We also provide back the specific matched annotation for the above input. We do this in order
  // to support more complex Annotation queries in the And message below. For example if we match
  // the search results to a region in your input, or a frame in a video input, this annotation
  // field will be that matched annotation info and the input will be the image/video that the user
  // originally added which contains those regions / frames.
  Annotation annotation = 3;
}

// This is the common building block of a query which is a sequence of And messages ANDed together.
// Note that some fields are used too RANK results (affect the scores) and some are used to FILTER
// results (unordered subset of your app's contents). In general, FILTER operations are more
// efficient queries at scale and when combined with RANK operations can speed up search performance
// as you effectively operate on a smaller sub-set of your entire app.
message And {
  // FILTER by input.data... information.
  // This can include human provided concepts, geo location info, metadata, etc.
  // This is effectively searching over only the trusted annotation attached to an input in your
  // app. To search by more specific annotation fields use the Annotation object here.
  Input input = 1;
  // RANK based predicted outputs from models such as custom trained models, pre-trained models,
  // etc. This is also where you enter the image url for a visual search because what we're asking
  // the system to do is find output embedding most visually similar to the provided input (that
  // input being in And.output.input.data.image.url for example). This will return the Hits
  // sorted by visual similarity (1.0 being very similar or exact match and 0.0 being very
  // dissimlar). For a search by Output concept, this means we're asking the system to rank
  // the Hits by confidence of our model's predicted Outputs. So for example if the model
  // predicts an image is 0.95 likely there is a "dog" present, that should related directly
  // to the score returned if you search for Output concept "dog" in your query. This provides
  // a natural ranking to search results based on confidence of predictions from the models and
  // is used when ANDing multiple of these types of RANK by Output queries together as well.
  Output output = 2;
  // If True then this will flip the meaning of this part of the
  // query. This allow for queries such as dog AND ! metadata=={"blah":"value"}
  bool negate = 3;

  // FILTER by annotation information. This is more flexible than just filtering by
  // Input information because in the general case each input can have several annotations.
  // Some example use cases for filtering by annotations:
  // 1) find all the inputs annotated "dog" by worker_id = "XYZ"
  // 2) find all the annotations associated with embed_model_version_id = "123"
  // 3) find all the annotations that are trusted, etc.
  //
  // Since all the annotations under the hood are joined to the embedding model's annotation
  // using worker_id's of other models like cluster models or concept models should be
  // combinable with queries like visual search (a query with Output filled in).
  Annotation annotation = 4;
}




// This is the search query used in /searches, model training requests, bulk data exports, etc.
message Query {
  // The query syntax is simply a list of And operatiosn that will be ANDed together to fetch
  // results which are returned to the user as Hit messages.
  repeated And ands = 1;

  // This allows the query to override any default language the app was setup in when doing Concept
  // based searches. This currently only affects public Models Output searches when those public
  // Models have translations for their Concepts.
  string language = 2;

  // filters in this query
  // e.q. only fetch annotations that have certain metadata
  repeated Filter filters = 3;

  // rankings in this query
  // e.g. visual search by a url
  repeated Rank ranks = 4;
}

// This is the new Search object used in saved searches.
message Search {
  // Search query.
  Query query = 1;

  // Customer facing, external ID for search to be saved. Provided by the user, e.g. "saved-search-1.
  // It is unique per application.
  string id = 2;

  // Application that owns this saved search.
  string application_id = 3;

  // Human readable display name of the saved search.
  string name = 4;

  // "As of" timestamp, indicating a time in the past as of which we want to
  // retrieve the annotations satisfying the query.
  google.protobuf.Timestamp as_of = 5;

  // Git hash of the code that ran the filter.
  string git_hash = 6;

  // When the saved search was created.
  google.protobuf.Timestamp created_at = 7;

  // When the saved search was updated.
  google.protobuf.Timestamp modified_at = 8;

  // The search algorithm to be used.
  // Options are are 'nearest_neighbor', 'brute_force', and 'avg_concept_brute_force'
  // The last two perform a brute force search visual search instead of a more scalable distributed
  // nearest neighbor search and should be used by advanced users only.
  // If not specified we default to nearest neighbor
  string algorithm = 9;

  // If true, save this search, and exit without executing the search.
  // If false execute the query
  bool save = 10;

  // Minimum value of confidence threshold score in result.
  // Defaults to 0.0 which means we won't do any thresholding as all probabilities will
  // likely be > 0.0.
  float min_value = 11;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 12;
}

message Filter {
  // If True then this will flip the meaning of this part of the
  // query. This allow for queries such as dog AND ! metadata=={"blah":"value"}
  bool negate = 3;

  // FILTER by annotation information.
  Annotation annotation = 4;

  // FILTER by input information.
  // For example you can filter inputs by status,
  Input input = 5;

  // Filter by annotation last updated time range.
  TimeRange last_updated_time_range = 6;
}

message TimeRange {
  google.protobuf.Timestamp start_time = 1; // Begin of the time range, optional, inclusive.
  google.protobuf.Timestamp end_time = 2; // End of the time range, optional, inclusive.
}

message Rank {
  // If True then this will flip the meaning of this part of the
  // query. This allow for queries such as !dog
  bool negate = 3;

  // RANK by annotation information.
  Annotation annotation = 4;
}

message AnnotationSearchMetrics {
  // The ground truth we are evaluating against
  clarifai.api.Search ground_truth = 1;

  // The set we are evaluating
  clarifai.api.Search search_to_eval = 2;

  // The metric result
  EvalMetrics metrics = 3;

  // data is filled out with the concepts used for this evaluation
  Data data = 4;

  // active_concept_count is the number of concepts for this evaluation
  uint32 active_concept_count = 5;


  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 6;
}










////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/text.proto
////////////////////////////////////////////////////////////////////////////////
message Text {
  // This is a raw text string.
  string raw = 1;
  // Url to a text file
  string url = 2;
  bool allow_duplicate_url = 3;
  // The hosted field lists original text hosted in Clarifai storage. This field is currently used
  // only in response.
  HostedURL hosted = 4;
}








enum APIEventType {
  API_EVENT_TYPE_NOT_SET = 0;

  // On Prem event types
  ON_PREM_PREDICT = 1;
  ON_PREM_TRAIN = 2;
  ON_PREM_SEARCH = 3;

  // Platform event types
}




enum UsageIntervalType {
  // undef UsageIntervalType is so that the interval field can be forced to be included
  undef = 0;
  day = 1;
  month = 2;
  year = 3;
}





////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/user.proto
////////////////////////////////////////////////////////////////////////////////
message User {
  reserved 13;

  string id = 1;

  string primary_email = 2 [deprecated = true];
  string first_name = 3;
  string last_name = 4;
  string company_name = 5;
  string job_title = 19;
  string job_role = 20;

  string bill_type = 7 [deprecated = true];

  // When the user was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 6;
  google.protobuf.Timestamp date_gdpr_consent = 8 [deprecated = true];
  google.protobuf.Timestamp date_tos_consent = 9 [deprecated = true];
  google.protobuf.Timestamp date_marketing_consent = 10 [deprecated = true];

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 11 [deprecated = true];
  repeated EmailAddress email_addresses = 12 [deprecated = true];
  bool is_org_admin  = 14 [deprecated = true];
  bool two_factor_auth_enabled = 15 [deprecated = true];
  uint32 teams_count = 16 [deprecated = true];

  // Is starred by the requesting user (only showed on get/list requests)
  // Please use PostUserStars/DeleteUserStars endpoints to star/unstar an user
  bool is_starred = 21;
  // How many users have starred the user (only showed on get/list requests)
  // Computed value, not editable
  int32 star_count = 22;


  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 17;

  // This is all the personal information of a user. GetUser/ListUsers will not return this
  // information unless the caller has the UserAccounts_Get scope on their key or is the user
  // themselves.
  UserDetail user_detail = 18;
}

// This message holds the confidential information from the User object that we don't want to expose
// to other users. It will be accessible only from /users/{user_id}/account and with the User scopes.
message UserDetail {
  string primary_email  = 1;
  string bill_type = 2;
  google.protobuf.Timestamp date_gdpr_consent = 3;
  google.protobuf.Timestamp date_tos_consent = 4;
  google.protobuf.Timestamp date_marketing_consent = 5;
  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 6;
  repeated EmailAddress email_addresses = 7;
  bool is_org_admin  = 8;
  bool two_factor_auth_enabled = 9;
  uint32 teams_count = 10;
  string country = 11;
  string state = 12;
}

message EmailAddress {
  string email = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  bool primary = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  bool verified = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
}



message Password {
  // TODO(lawrence): merge this with message UserPassword above
  // unencrypted password string
  string plaintext = 1;
}





message PasswordViolations {
  // when new password length is shorter than minimum length set
  bool minimum_length = 1;
  // when new password length is longer than maximum length set
  bool maximum_length = 2;
  // there is no upper case letter in the new password when there should be at least one
  bool upper_case_needed = 3;
  // there is no lower case letter in the new password when there should be at least one
  bool lower_case_needed = 4;
  // there is no numerics in the new password when there should be at least one
  bool numeric_needed = 5;
  // there is no special character in the new password when there should be at least one
  bool non_alphanumeric_needed = 6;
  // when one of the N most recent old password is reused, N is specified by password_reuse_epoch in db.password_policies
  bool password_reuse = 7;
  // when either user's first, middle or last name is used in the new password
  bool exclude_names = 8;
  // when first part of user's email (exact string or after removing special characters) is used in the new password
  bool exclude_email = 9;
  // when there are confusing letters in the new password, such as o (first character of 'omega') vs 0 (zero)
  bool no_confusing_letters = 10;
  // when there are simple password patterns used, such as 12345678 or aaaaaaa1
  bool no_simple_passwords = 11;
  // when there are common vocabs from the common vocab list used
  bool no_common_vocabs = 12;
  // when the current password is contained in the new password or vice versa
  bool no_overlap_with_old = 13;
  // when password has to be changed becauase it's too old
  bool password_lifespan = 14;
}


////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/video.proto
////////////////////////////////////////////////////////////////////////////////
message Video {
  // This is a URL to a publicly accessible video file. The platform will download this file server
  // side and then process.
  string url = 1;
  // The base64 field is using video file bytes directly in the request.
  // NOTE: if you're sending a json request, then this MUST be base64 encoded before sending (hence
  // the name here).
  // When using our grpc clients, you DO NOT need to base64 encode
  // it yourself since the clients know how to do this for you automatically and will avoid the
  // base64 encoding if they send a binary request.
  bytes base64 = 2;
  bool allow_duplicate_url = 4;

  // URL of thumbnail image, which is currently frame at position of 1s. This field is currently
  // used only in response.
  string thumbnail_url = 5;
  // The hosted field lists original video hosted in Clarifai storage. This field is currently used
  // only in response.
  HostedURL hosted = 6;
}








////////////////////////////////////////////////////////////////////////////////



////////////////////////////////////////////////////////////////////////////////
// Messages from /proto/clarifai/api/workflow.proto
////////////////////////////////////////////////////////////////////////////////
message Workflow {
  // The workflows's unique id.
  string id = 1;
  // The app the workflow belongs to
  string app_id = 2;

  // When the workflow was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 3;

  // The list of nodes retrieved from latest workflow version.
  // Each node can specify an input node that it connects to in order to define the graph.
  repeated WorkflowNode nodes = 4;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 5;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 6;

  // The user the workflow belongs to
  string user_id = 7;

  // When the workflow was last modified
  google.protobuf.Timestamp modified_at = 8;

  // Info about the workflow version
  WorkflowVersion version = 9;

  // Is starred by the requesting user (only showed on get/list requests)
  // Please use PostWorkflowStars/DeleteWorkflowStars endpoints to star/unstar a workflow
  bool is_starred = 10;
  // How many users have starred the workflow (only showed on get/list requests)
  // Computed value, not editable
  int32 star_count = 11;
}

message WorkflowVersion {
  // Id of this version.
  string id = 1;

  // Workflow id for this version.
  string workflow_id = 2;

  // When the version was created.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 3;

  // Most recent time when the version was updated.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp modified_at = 4;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 5;

  // The list of nodes that make up the workflow version. Each node can specify an input node
  // that it connects to in order to define the graph.
  repeated WorkflowNode nodes = 6;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 7;

  // The app the workflow version belongs to.
  string app_id = 8;
  // The user the workflow version belongs to.
  string user_id = 9;
}

message WorkflowNode {
  // An identifier for this node in the graph. This is used when connecting NodeInputs
  // together.
  string id = 1;

  // The model that will do the processing at this node. We only vlidate the model.id and
  // model.model_version.id fields.
  Model model = 2;

  // Each WorkflowNode can connect to multiple input nodes so that we can handle multi-model data
  // and more complex workflow operations.
  repeated NodeInput node_inputs = 3;
  // suppress the output for workflow prediction
  bool suppress_output = 4;
}

// NodeInput represents inputs to a node of the graph.
message NodeInput {
  // The id to a connected WorkflowNode which will be used as an input for current WorkflowNode.
  string node_id = 1;
}

message WorkflowResult {
  string id = 1;
  clarifai.api.status.Status status = 2;
  // When the object was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 3;
  Model model = 4;
  Input input = 5;
  repeated Output outputs = 6;
  // Indicate if the output of this model is suppressed.
  bool suppress_output = 7;
}


message WorkflowState {
  // A unique ID for the workflow state.
  // To start saving a state in a PostWorkflowResults request set this ID to "init"
  // and it will return a newly generated unique state id that you can then pass in subsequent
  // PostWorkflowResults calls. These state expire after 5 minutes between calls.
  string id = 1;
}




////////////////////////////////////////////////////////////////////////////////

////////////////////////////////////////////////////////////////////////////////
// App Duplication
////////////////////////////////////////////////////////////////////////////////


message AppDuplication {
  //the id of app duplication
  string id = 1;
  //the id of new app
  string new_app_id = 2;
  //the name of new app
  string new_app_name = 3;
  //the status of app duplication
  clarifai.api.status.Status status = 4;
  //when is the app duplication triggered
  google.protobuf.Timestamp created_at = 5;
  //The last time when is the status got updated
  google.protobuf.Timestamp last_modified_at = 6;
  // Only copy resources depending on the filters
  AppDuplicationFilters filter = 7;
}

message AppDuplicationFilters {
  // Copy only inputs and default annotations
  bool copy_inputs = 1;
  // Copy only concepts
  bool copy_concepts = 2;
  // Copy annotations and what it depends on: inputs and concepts
  bool copy_annotations = 3;
  // Copy models and what it depends on: concepts
  bool copy_models = 4;
  // Copy workflows and what it depends on: models and concepts
  bool copy_workflows = 5;
}






////////////////////////////////////////////////////////////////////////////////
// Tasks
////////////////////////////////////////////////////////////////////////////////

// Task is the work that needs to be done for labeling the inputs in an app.
message Task {
  // Unique ID for the task.
  string id = 1;

  // When the task was created.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 2;

  // Most recent time when the task was updated.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp modified_at = 3;

  // Task type.
  TaskType type = 4;

  // Description of the task.
  string description = 5;

  // Worker details.
  TaskWorker worker = 6;

  // List of concept ids used in the work of this task if label type is classification.
  repeated string concept_ids = 7;

  // List of inputs used in this task will be taken from this source.
  TaskInputSource input_source = 8;

  // For model predictions on video: Sample delay for video predicting (1 frame per N milliseconds)
  uint32 sample_ms = 9;

  // AI assistant details.
  TaskAIAssistant ai_assistant = 10;

  // Review details.
  TaskReview review = 11;

  // Status of this task.
  clarifai.api.status.Status status = 12;

  // Add a title for this task to quickly recognise it in a list of tasks.
  string name = 13;

  AiAssistParameters ai_assist_params = 14;

  enum TaskType {
    TYPE_NOT_SET = 0;

    // Concepts classification tasks annotate concepts for the overall image, frame of video or section of text.
    CONCEPTS_CLASSIFICATION = 1;
    // Bounding box detection tasks annotate rectangular bounding box regions around each concept in an image, frame of video or section of text.
    BOUNDING_BOX_DETECTION = 2;
    // Polygon detection tasks annotate free-form regions around concepts in an image, frame of video or section of text.
    POLYGON_DETECTION = 3;
  }

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 15;

  // The app the task belongs to.
  string app_id = 16;
  // The user the task belongs to.
  string user_id = 17;
}

message AiAssistParameters {
  // Min and max threshold values for approving annotations by default based on prediction score
  float min_threshold = 1;
  float max_threshold = 2;
  // ids of concept relations. Used in AI assist workflow
  repeated string concept_relation_ids = 3;
}

message TaskWorker {
  // Worker strategy.
  TaskWorkerStrategy strategy = 1;

  // Who will work on this task.
  repeated string user_ids = 2;

  // Info based on the worker strategy,
  oneof strategy_info {
    TaskWorkerPartitionedStrategyInfo partitioned_strategy_info = 3;
  }

  enum TaskWorkerStrategy {
    reserved 1;

    WORKER_STRATEGY_NOT_SET = 0;

    // The inputs will be partitioned in several partitions.
    // Each worker will label one or more input partitions.
    PARTITIONED = 2;

    // Each worker will label all inputs from input source.
    FULL = 3;
  }
}

message TaskWorkerPartitionedStrategyInfo {
  // Define how the partitioning should work.
  TaskWorkerPartitionedStrategy type = 1;

  // How many workers will label each input.
  int32 workers_per_input = 2;

  // In case of weighted partitioning, map user ids to weights.
  // Each labeler will be assigned work proportional to its own weight as compared to the sum of total weight.
  //
  // EXAMPLE:
  // If we have 3 workers, and weights = {1: 30, 2: 30, 3: 40},
  // then first worker will have assigned 30% of the work,
  // second worker will have assigned 30% of the work,
  // and third worker will have assigned 40% of the work.
  // You may use weights which add up to 100, but it's not necessary.
  // For example, weights {1: 30, 2: 30, 3: 40} are equivalent with {1: 3, 2: 3, 3: 4}
  // because they represent the same percentages: {1: 30%, 2: 30%, 3: 40%}.
  //
  // NOTE:
  // Note that no worker should be assigned a weight percentage greater than 1/workers_per_input.
  // It is mathematically impossible to partition the work in such a case.
  // Why? Say, we have 3 workers. And workers_per_input = 2, i.e. each input must be labeled by 2 workers.
  // Let's assign weights {1: 51%, 2: 25%, 3: 24%}.
  // Note that first worker has a weight percentage higher than 1/workers_per_input = 1/2 = 50%.
  // If we have 100 inputs, then a total of 100 * workers_per_input = 200 cumulative inputs will be labeled by these 3 workers.
  // Worker 1 should label 102 cumulative inputs, while worker 2 and worker 3 will label 98 cumulative inputs together.
  // No matter how we assign the 98 cumulative inputs, the 2 workers will be able to label up to 98 actual inputs.
  // This means the remaining 2 inputs will be labeled only by worker 1. This contradicts the worker_per_input = 2 requirement.
  google.protobuf.Struct weights = 3;

  enum TaskWorkerPartitionedStrategy {
    PARTITIONED_WORKER_STRATEGY_NOT_SET = 0;

    // Each worker will label (approximately) the same number of inputs.
    EVENLY = 1;

    // Each worker will have an assigned weight.
    // See weights field for more details.
    WEIGHTED = 2;
  }
}

message TaskInputSource {
  // Type of input source.
  TaskInputSourceType type = 1;

  // If type is SAVED_SEARCH, then this is the saved search id.
  string id = 2;

  enum TaskInputSourceType {
    INPUT_SOURCE_TYPE_NOT_SET = 0;

    // Use all inputs in the app.
    ALL_INPUTS = 1;
    // Use the inputs from a saved search.
    SAVED_SEARCH = 2;
  }
}

message TaskReview {
  // Task review strategy.
  TaskReviewStrategy strategy = 1;

  // Who will review this task.
  repeated string user_ids = 2;

  // Info based on the review strategy,
  oneof strategy_info {
    TaskReviewManualStrategyInfo manual_strategy_info = 3;
    TaskReviewConsensusStrategyInfo consensus_strategy_info = 4;
  }

  enum TaskReviewStrategy {
    TASK_REVIEW_STRATEGY_NOT_SET = 0;

    // No review is needed.
    NONE = 1;

    // Manual review strategy.
    MANUAL = 2;

    // Consensus review strategy.
    CONSENSUS = 3;
  }
}

message TaskReviewManualStrategyInfo {
  // This field represents the percentage of inputs that will be reviewed by reviewers. It is a value between 0 and 1.
  float sample_percentage = 1;
}

message TaskReviewConsensusStrategyInfo {
  reserved 1;

  // The number of labelers that need to agree in order to automatically approve an annotation.
  uint32 approval_threshold = 2;
}

message TaskAIAssistant {
  // The worker is helped by an AI assistant.
  // This field is the workflow id which is used to assist the worker with predictions.
  // If empty, then AI assistant is disabled.
  string workflow_id = 1;
}




// TaskStatusCountPerUser can represent -
//    1. count of human created annotations for a user for each valid status
//    2. count of inputs (anchor annotation) for a user for each valid status

message TaskStatusCountPerUser {
  string user_id = 1;
  uint32 pending = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 awaiting_review = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 success = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 review_denied = 5 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 awaiting_consensus_review = 6 [(clarifai.api.utils.cl_show_if_empty) = true];
}

enum RoleType {
  TEAM = 0;
  ORG = 1;
}










////////////////////////////////////////////////////////////////////////////////
// Collectors
////////////////////////////////////////////////////////////////////////////////

// Collector is a data pathway from a CollectorSource to an app to collect data automatically.
// For example, a CollectorSource
message Collector {
  // Unique ID for the collector.
  string id = 1;

  // Human readable description for the collector.
  string description = 2;

  // When the collector is created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 3;

  // This is a workflow to run inline in model predict calls. It should ONLY have very fast and
  // light-weight models in it as it will effect the speed of the predictions being made.
  // This workflow's purpose is to filter down the inputs to queue for the collector to process.
  // The input to this workflow is going to be the OUTPUT of the model, not the input to the model
  // since we want to encourage having fast workflows that can also take advantage of the model
  // outputs to make deciions (for example: thresholding based on concepts). If the workflow
  // output has any field that is non-empty then the input will be queued for the collector
  // to process with the post_queue_workflow_id.
  string pre_queue_workflow_id = 4;

  // A workflow to run to after the collector is processing the queued input. This workflow
  // uses the original input to the model as input to the workflow so that you can run additional
  // models as well on that input to decide whether to queue the model or not. If the workflow
  // output has any field that is non-empty then it will be passed on to POST /inputs to
  // the destination app.
  string post_queue_workflow_id = 5;

  // The source of the collector to feed data into this app.
  // Note(zeiler): if we wanted more than one source per collector we could make this it's own
  // object and introduce /collectors/{collector_id}/sources
  // We will keep it simple for now and have just one source per collector since a user can make
  // more than one collector in the same app anyways.
  CollectorSource collector_source = 6;

  // This is the workflow ID to do POST /inputs with the collected data using.
  // This needs to be present at all times in this app for the collector to work.
  // If this is not specified then it will use the default_workflow_id of the app.
  // Note(zeiler): not yet available, uses only the default workflow that POST /inputs uses.
  // string workflow_id = 7;


  // Status for the collector. This allows you to pause a collector without having to delete it as
  // an example.
  clarifai.api.status.Status status = 7;

}

// Configuration for the source to collect data from.
// Only one of the fields can be present at a time.
message CollectorSource {
  // The ID of the source in case we want to implment /collectors/{collector_id}/sources
  // string id = 1;

  // Collect from the inputs passed in for PostModelOutputs predictions of a specific model.
  // This does not apply to models used within workflows, only PostModelOutputs calls.
  APIPostModelOutputsCollectorSource api_post_model_outputs_collector_source = 2;
}



// This is configuration for using the inputs send for model prediction in our API as
// as the source for data.
message APIPostModelOutputsCollectorSource {
  // To define the model that we should collect from we need to specify the following 4 IDs:
  // The User ID of the model we want to collect from.
  // This is User B in the example.
  string model_user_id = 1;
  // The App ID of the model we want to collect from.
  string model_app_id = 2;
  // The Model ID of the model we want to collect from.
  string model_id = 3;
  // The Version ID of the model we want to collect from.
  string model_version_id = 4;

  // This key is used to POST /inputs into your app by the collector. It can be an API key or a
  // PAT. This needs the permissions that are needed for POST /inputs for the app_id this
  // Collector is defined in.
  string post_inputs_key_id = 5;

  // The most flexible scenario is User C creates a collector and she wants to ingest User A's
  // predictions of User B's model into their app (User C's app), for which User C has created
  // the annotation workflow using a combination of models, perhaps from User D even.

  // The User ID of the caller of the model we want to collect from.
  // This is needed because the below Model's ids could be used by multiple users like the
  // clarifai/main models are or any model that has been shared with a collaborator. Therefore we
  // need to know which caller of the model to collect inputs from.
  // This is User A in the example.

  // This is a private field that defaults to the app owner for public users.
  // If this is left blank then this collector will collect from ALL users calling the given model.
}





////////////////////////////////////////////////////////////////////////////////
// Stats Collection Objects.
////////////////////////////////////////////////////////////////////////////////
message StatValue {
  // The time of the event. Defaults to now().
  google.protobuf.Timestamp time = 1;

  // A value for the metric you're recording.
  float value = 2;

  // List of tags to attach to this stat. Each should contain one colon so that the first part will
  // be used as a tag group while the second being the tag itself. For example: ["task_id:a",
  // "worker_id:1"]. These tag groups like "task_id" or "worker_id" are important for aggregating
  // values in the StatValueAggregateQuery.
  repeated string tags = 3;
}



message StatValueAggregateResult {
  // The list of repeated aggregate values and their counts.
  repeated StatValueAggregate stat_value_aggregates = 1;

  // The query that created these results.
  StatValueAggregateQuery stat_value_aggregate_query = 2;
}



message StatValueAggregate {
  // The time of the aggregation. For example, if you aggregate over "HOUR" buckets then you can
  // expect each hour that has atleast one value (matching the rest of your query fields) will have
  // a StatValueAggregate with the time filled into that hour.
  google.protobuf.Timestamp time = 1;
  // The value aggregated according to the stat_value_agg_type
  float aggregate_value = 2;
  // The count of the stat values that were used in this aggregation.
  uint64 count = 3;
  // The tags for this aggregated_value and count. This will be filled in if tag groups were used in
  // the query to group aggregations.
  repeated string tags = 4;
}



message StatValueAggregateQuery {
  // These tags are used to filter down the values before they are aggregated. For example,
  // if you want to aggregate values for "task_id:a" you could specify that as a tag here.
  repeated string tags = 1;

  // These are tag groups to aggregate over. So for example if you added stat values with tags
  // "task_id:a" and others with "task_id:b", then added ["task_id"] to the task group, it the
  // aggregation would return StatValueAggregate values for each task_id. If you provide more than
  // one tag_group the response will return all rolled up combinations of them. For example
  // ["task_id", "something"] where "something:1" and "something:2" were used as tags for some
  // values then you'd get StatValueAggregate values back for:
  // task_id | something
  // a       | 1
  // a       | 2
  // b       | 1
  // b       | 1
  repeated string tag_groups = 2;

  // Aggregation function to use over the values. Count(value) is also always returns.
  // Defaults to 'sum' if not provided.
  StatValueAggType stat_value_agg_type = 3;

  // Aggregation bins for time where the values will be aggregated at this bin granualarity.
  // And the "time" field will be returned in StatValueAggregate object.
  // If not provided then bins are not used, and all time is aggregated over.
  StatTimeAggType stat_time_agg_type = 4;

  // If provided the time range over which values will be >= this time. If not provided then
  // all values will be used back to start of time.
  google.protobuf.Timestamp start_time = 5;

  // If provided the time range over which values will be <= this time. If not provided then all
  // values will be used up until now().
  google.protobuf.Timestamp end_time = 6;
}
enum StatValueAggType {
  SUM = 0;
  AVG = 1;
}

enum StatTimeAggType {
  NO_TIME_AGG = 0;
  YEAR = 1;
  MONTH = 2;
  WEEK = 3;
  DAY = 4;
  HOUR = 5;
  MINUTE = 6;
}




/*
    SDK Billing Cycle defines which year and month is associated with the billing data
*/















/////////////////////////////
// Visibilty of the resource.
/////////////////////////////
// Visibility represents how visible the given resource is to other users.
// When authenticating a request we can tell if a user is a collaborator or a teammate for the
// the app that contains the resource and set their allowed visibility. We use that to restrict
// what they are allowed to see:
// If AllowedVisibility is PRIVATE then we allow PRIVATE (10), ORG (30), PUBLIC (50)
// If AllowedVisibility is ORG then we allow ORG (30), PUBLIC (50)
// If AllowedVisibility is PUBLIC then we allow PUBLIC (50) only.
message Visibility {
  // Gettable defined the level of access for GET operations for this resource.
  enum Gettable {
    // Default value not allowed.
    UNKNOWN_VISIBILITY = 0;
    // PRIVATE requires collaborator or team permissions in order to GET this resource.
    PRIVATE = 10;
    // ORG requires you to be in the same org in order to GET this resource, but don't have to be a
    // teammate or collaborator.
    ORG = 30;
    // PUBLIC opens up GET access to the resource to any user on the platform even if they are not
    // a teammate or collaborator.
    PUBLIC = 50;
  }
  Gettable gettable = 1;
}

message TrendingMetric {
  string user_id = 1;
  string app_id = 2;
  string object_id = 3;
  uint64 view_count = 4;
}



enum ValidationErrorType {
  VALIDATION_ERROR_TYPE_NOT_SET = 0;

  RESTRICTED = 1;
  DATABASE = 2;
  FORMAT = 3;
}






message TimeSegment {
  // A unique id for the time segment.
  string id = 1;

  Data data = 2;

  TimeInfo time_info = 3;
}

message TimeInfo {
  // Number of frames
  uint32 num_frames = 1;
  // Timestamp where track begins.
  uint32 begin_time = 2;
  // Timestamp where track ends.
  uint32 end_time = 3;
}



