syntax = "proto3";

import "proto/clarifai/api/status/status.proto";
import "proto/clarifai/api/status/status_code.proto";
import "proto/clarifai/api/utils/extensions.proto";
import "proto/clarifai/api/utils/matrix.proto";
import "proto/clarifai/auth/util/extension.proto";

import "google/protobuf/struct.proto";
import "google/protobuf/timestamp.proto";
import "google/protobuf/wrappers.proto";

package clarifai.api;

option go_package = "github.com/Clarifai/clarifai-go-grpc/proto/clarifai/api";
option java_multiple_files = true;
option java_package = "com.clarifai.grpc.api";
option objc_class_prefix = "CAIP";


// Annotation of an asset with metadata
message Annotation {
  reserved 4, 5, 6, 11, 12;

  // The ID for the annotation
  string id = 1;

  // ID of the input this annotation is tied to
  string input_id = 2;

  // The data passed along in this annotation.
  Data data = 3;

  // task_id is deprecated in annotation_info. Use task_id
  google.protobuf.Struct annotation_info = 13;

  // ID of the user this annotation is created by
  string user_id = 15;
  // ID of the model version this annotation is created by
  string model_version_id = 16;

  // DEPRECATED.
  string embed_model_version_id = 14 [deprecated = true];

  // Annotation Status
  clarifai.api.status.Status status = 7;

  // When the annotation was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 8;

  // When the annotation was modified.
  google.protobuf.Timestamp modified_at = 9;

  // Whether or not this annotation is trusted
  // Will be deprecated
  bool trusted = 10 [deprecated = true];

  // Is this the input level annotation.
  bool input_level = 17;

  // Consensus review related information, e.g.
  // * annotation group
  // * id of annotation parent, in case the annotation was split from another annotation
  google.protobuf.Struct consensus_info = 18;
  // The id of the task annotation belongs to
  string task_id = 19;
  // ID of the workflow version this annotation is created by
  string workflow_version_id = 20;
}

// Worker is the author of an annotation.
message Worker {
  oneof worker {
    // User is the human that created the annotation.
    //
    // By default no real names of users are returned in responses. These can
    // be requested with the 'names' additional field.
    User user = 1;

    // Model is the model that created the annotation.
    Model model = 2;

    // Workflow is the workflow that created the annotation.
    Workflow workflow = 3;
  }
}

// Application with tasks and datasets
message App {
  reserved 10, 11, 12;
  string id = 1;
  string name = 2;
  string default_language = 3;
  // Default workflow id deprecated in favor of default_workflow
  string default_workflow_id = 4;
  Workflow default_workflow = 23;
  // why is user_id present here when this message type is used in PostApps but completely ignored there? PostApp already
  // specifies the userid in path but doesn't even actually use neither of userids, it instead used the id from auth context.
  // This creates a lot of ambiguity, should always have different message types for Post/Get endpoints so that the minimum interface for each op can be described
  string user_id = 5;
  // When the app was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 6;
  // When the app was last modified
  google.protobuf.Timestamp modified_at = 17;
  // if user accept legal consent for face recognition
  uint32 legal_consent_status = 7;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 13;

  // short description about the app.
  string description = 14;

  // Default value for model predictions on video: Sample delay for video predicting (1 frame per N milliseconds)
  uint32 sample_ms = 15;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 16;

  // data tier id this app is using.
  string data_tier_id = 18;

  // Is starred by the requesting user (only showed on get/list requests)
  // Please use PostAppStars/DeleteAppStars endpoints to star/unstar an app
  bool is_starred = 19;
  // How many users have starred the app (only showed on get/list requests)
  // Computed value, not editable
  int32 star_count = 20;

  // Notes for the application
  // This field should be used for in-depth notes and supports up to 64Kbs.
  string notes = 21;

  // Representative image for this app
  Image image = 22;

  // An app marked as a template can be duplicated by any user that can see it,
  // including all visible resources within it.
  google.protobuf.BoolValue is_template = 25;

  AppExtraInfo extra_info = 24;
}

message AppExtraInfo {
  // Revision marker for this application.
  // The value of the revision changes when
  // * inputs are added, updated or deleted
  // * annotations are added, updated or deleted
  // * inputs are added to or removed from datasets
  // For example, this value can be used to detect if client side caches related to searching should be invalidated.
  // Field not filled in for list endpoints, use GetApp
  string search_revision_marker = 1;
}

// App query
message AppQuery {
  // Query by application name. This supports wildcard queries like "gen*" to match "general" as an example.
  string name = 1;
}

// Collaborator - invited user, who shares an access to an application
message Collaborator {
  // id of this collaborator
  string id = 1;
  // the app this collaborator has access to
  // FIXME(zeiler): this should be in the user_app_id.app_id already from the endpoint.
  clarifai.api.App app = 2;
  // who is this collaborator
  clarifai.api.User user = 3;
  // the permission this collaborator
  repeated string scopes = 4;
  repeated string endpoints = 5;

  // When the app was shared with. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 6;

  // When the collaborator was updated.
  google.protobuf.Timestamp modified_at = 7;

  // When the collaborator was removed from app.
  google.protobuf.Timestamp deleted_at = 8;
}

// collaboration includes an app you're invited to work on.
message Collaboration {
  // the application
  App app = 1;
  // the app owner's info(including user_unique_id, first_name, last_name, primary_email)
  User app_owner = 2;
  // the low-level scope users are shared with for this collaboration
  repeated string scopes = 3;
  // the endpoint-level scopes users are shared with for this collaboration
  repeated string endpoints = 4;
  // when is the collaboration created
  google.protobuf.Timestamp created_at = 5;
}

// Audio asset struct
message Audio {
  // This is a URL to a publicly accessible image file. The platform will download this file server
  // side and then process.
  string url = 1;
  // The base64 field is using image file bytes directly in the request.
  // NOTE: if you're sending a json request, then this MUST be base64 encoded before sending (hence
  // the name here).
  // When using our grpc clients, you DO NOT need to base64 encode
  // it yourself since the clients know how to do this for you automatically and will avoid the
  // base64 encoding if they send a binary request.
  bytes base64 = 2;
  // If True then you will be allowed to have multiple urls.
  bool allow_duplicate_url = 4;
  // The hosted field lists original audio hosted in Clarifai storage. This field is currently used
  // only in response.
  HostedURL hosted = 5;
  // audio info
  AudioInfo audio_info = 6;
}

message AudioInfo {
  // audio format
  string audio_format = 1;
  // sample rate
  int32 sample_rate = 2;
  // audio track duration in seconds
  float duration_seconds = 3;
  // audio track bit rate
  int32 bit_rate = 4;
}

// Track proto encodes information of a track over a number of frames
message Track {
  reserved 3;
  // track id
  string id = 1;

  // This is a recursive definition which can contain all the concepts,
  // embeddings, etc. that are computed within this track.
  Data data = 2;

  TimeInfo time_info = 4;

  float quality = 5;
}





// Cluster data
message Cluster {
  string id = 1;

  // Number of annotations tied to the cluster in the app
  uint32 count = 2;

  // The score assigned to this cluster.
  // For List Clusters endpoint, this represents percentage of inputs in the app assigned to this cluster.
  float score = 3;

  // Representative hits for cluster (for now we only return 1)
  repeated Hit hits = 4;

  repeated float projection = 5;
}

// Color data
message Color {
  string raw_hex = 1;
  W3C w3c = 2;
  float value = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
}

message W3C {
  string hex = 1;
  string name = 2;
}

// Common message to identify the app in a url endpoint.
message UserAppIDSet {
  // Note user_id 'me' is reserved - it is the alias for the id of authorized user
  string user_id = 1;
  string app_id = 2;
}

// PatchAction
message PatchAction {
  // The operation to perform on the patched metadata given a path
  // For now only operations 'overwrite', 'delete, and 'merge' is supported
  string op = 1;

  // If the action is 'merge' and there is a conflict, how to resolve it.
  // The options are
  // 'overwrite_by_id', 'remove_by_id', 'merge_by_id','overwrite', 'append' and 'do_nothing'
  // Note that for conflict resolutions '*_by_id' to work on a list, the list should contain
  // objects with an 'id' field which will be used to uniquely identify each field. For example
  // Patching existing json
  // {
  //   "tag": [
  //     {
  //       "id": "1",
  //       "data": 1
  //     },
  //     {
  //       "id": "2",
  //       "data": 2
  //     }
  //   ]
  // }
  // with op 'merge' and merge_conflict_resolution 'overwrite_by_id'
  // {
  //   "tag": [
  //     {
  //       "id": "2",
  //       "data": 3
  //     }
  //   ]
  // }
  // would produce
  // {
  //   "tag": [
  //     {
  //       "id": "1",
  //       "data": 1
  //     },
  //     {
  //       "id": "2",
  //       "data": 3
  //     }
  //   ]
  // }
  // while with merge_conflict_resolution 'remove_by_id' it would produce
  // {
  //   "tag": [
  //     {
  //       "id": "1",
  //       "data": 1
  //     }
  //   ]
  // }
  //
  // Option 'append' will simply create a list on conflicts. For example in above example
  // the final result would be
  // {
  //   "tag": [
  //     {
  //       "id": "1",
  //       "data": 1
  //     },
  //     {
  //       "id": "2",
  //       "data": [2, 3]
  //     }
  //   ]
  // }
  string merge_conflict_resolution = 2;

  // Path for the change. For example 'tag[1].data' is a valid path in above example.
  // Default path is root level i.e. ''.
  string path = 3;
}
////////////////////////////////////////////////////////////////////////////////

// Concept or tag
message Concept {
  // The concept's unique id.
  string id = 1;
  // The name of the concept in the given language.
  string name = 2;
  // Used to indicate presence (1.0) or not (0.0) of this concept when making a request.
  // This is also the prediction probability when returning predictions from our API.
  // For convenience we use the default of 1.0 when making requests so the concept you provide is
  // is treated as a positive (1.0) and not a negative (which would be value == 0.0).
  float value = 3 [(clarifai.api.utils.cl_default_float) = 1.0, (clarifai.api.utils.cl_show_if_empty) = true];
  // When the concept was created. The format is https://www.ietf.org/rfc/rfc3339.txt .
  // Example: "2006-01-02T15:04:05.999999Z". This field is used only in a response.
  google.protobuf.Timestamp created_at = 4;

  // The language in which the concept name is in. This is *ONLY* used in the response and setting
  // it in a request is ignored since the default language of your app is used when creating
  // or patching a Concept. To set other languages for your concept use the ConceptLanguage object
  // and its corresponding endpoints.
  string language = 5;
  // The application id that this concept is within. This can be ignored by most users.
  string app_id = 6;
  // The definition for the concept. Similar to name. This can be ignored by most users.
  string definition = 7;
  // The vocabulary that this concept belongs to. This is useful if you have different unique sets
  // of concepts that you can separate out based on this field. For example "age_appearance" vs
  // "gender_appearance" in a list of concept returned from the demographics model.
  string vocab_id = 8;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 9;

  // The user the concept belongs to.
  string user_id = 10;

  // Information about keypoints for this concept
  KeypointInfo keypoint_info = 11;

  // Optional extra info.
  ConceptExtraInfo extra_info = 12;
}

message KeypointInfo {
  // Names of the keypoints
  repeated string keypoint_names = 1;

  // Defines the connections between keypoint_names. Each value represents the index in keypoint_names.
  repeated KeypointEdge skeleton = 2;
}

message KeypointEdge {
  uint32 k1 = 1;
  uint32 k2 = 2;
}

// ConceptExtraInfo represents extra information related to a concept that is context-dependent.
// It is only set when requested in ConceptExtraInfoRequest.
message ConceptExtraInfo {
  // Whether this concept is rankable based on ConceptExtraInfoRequest configuration.
  bool is_rankable = 1;
}

// ConceptCount
message ConceptCount {
  // The concept's unique id.
  string id = 1;
  // The name of the concept.
  string name = 2;
  // The total count for concepts labeled for all asset statues (processing, to_process, processed, error)
  ConceptTypeCount concept_type_count = 3;
  // The detail count for different assets status
  DetailConceptCount detail_concept_count = 4;
}

// ConceptTypeCount
message ConceptTypeCount {
  // The number of inputs that have a concept with a value of 1.0 (indicating presence of the
  // concept in an input).
  uint32 positive = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The number of inputs that have a concept with a value of 0.0 (indicating absence of the
  // concept in an input).
  uint32 negative = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
}

// DetailConceptCount
message DetailConceptCount {
  // The concept count for processed assets
  ConceptTypeCount processed = 1;
  // The concept count for to process assets
  ConceptTypeCount to_process = 2;
  // The concept count for assets with status error
  ConceptTypeCount errors = 3;
  // The concept count for processing assets
  ConceptTypeCount processing = 4;
}

// ConceptQuery
message ConceptQuery {
  // The name of the concept to search.
  string name = 1;
  // (optional) The language of the concept name in a search. Defaults to English.
  string language = 2;
  // (optional) The id of workflow. If no id is provided, then application base workflow is used.
  string workflow_id = 3;
}

// This represents a relation (i.e. edge) between the subject concept and the object concept
message ConceptRelation {
  // ID of the concept relation
  string id = 1;

  // The subject concept (i.e. source) of the concept relation
  Concept subject_concept = 2;

  // The subject concept (i.e. destination) of the concept relation
  Concept object_concept = 3;
  // The predicate (i.e. edge) linking the subject and the object
  // Both subject_concept and object_concept are concepts.
  // The predicate is the type of relationship.
  // That predicate acts on the subject.
  //
  // There are three current types of predicates:
  // 1) "hyponym"
  // 2) "hypernym"
  // 3) "synonym"
  //
  // 1) For example, 'hyponym' is a type of predicate which represents 'is_a_kind_of' relation so
  // the following relationship:
  // 'honey' (subject), 'hyponym' (predicate), 'food' (object)
  // Can more easily be read as:
  // 'honey' 'is a kind of' 'food'
  //
  //
  // 2) The 'hypernym' relation is the opposite of 'hyponym' and when you add one of the
  // relationships the opposite will automatically appear for you in queries.
  //
  // The 'hypernym' can be read as 'is a parent of' so:
  // 'food' (subject), 'hypernym' (predicate), 'honey' (object)
  // Can more easily be read as:
  // 'food' is a parent of 'honey'
  //
  // 3) The 'synonym' relation defines two concepts that essential mean the same thing. This
  // is more like a "is" relationship. So for example a 'synonym' relationship could be:
  // "puppy" is "pup"
  // The reverse is also true once the former is added so:
  // "pup" is "puppy"
  // will appear in queries as well.
  string predicate = 4;

  // The knowledge graph id that this edge belongs to. If using the app's global knowledge graph
  // and not a specific one then this should be the empty string "".
  string knowledge_graph_id = 5;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 6;
}

// A Knowledge Graph is a logical subsets of edges in the overall Concept Graph
message KnowledgeGraph {
  // ID of the knowledge graph
  string id = 1;
  // Name of the knowledge graph
  string name = 2;
  // Human readable description of the knowledge graph
  string description = 3;
  // The app that contains the images that correspond to the concepts in the knowledge graph
  string examples_app_id = 4;
  // The app that contains the sample images that we want to show the customer for the concepts in the knowledge graph
  string sampled_examples_app_id = 5;
}


// ConceptMappingJob
message ConceptMappingJob {
  // The id of the knowledge graph being used for this concept mapping job
  string knowledge_graph_id = 1;
  // The ids of the concepts being mapped
  repeated string concept_ids = 2;
}

// This represents a link to an outside source for the given concept.
// The values from here are sticked into Concept message into the name and definition fields when
// returning from the API in your default language. The "id" field here becomes the "language"
// field of the Concept message which is a little weird.
message ConceptLanguage {
  // This is the language code for the language such as "en".
  string id = 1;
  // The type of the outside source.
  string name = 2;
  // The ID that is referenced in the source.
  string definition = 3;
}


// Data
message Data {
  reserved 4, 10;
  // Input and output images.
  Image image = 1;
  // Input and output videos.
  Video video = 2;
  // A list of concepts.
  repeated Concept concepts = 3;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 5;
  // Geography information.
  Geo geo = 6;

  // The dominant colors within an image.
  repeated Color colors = 7;
  // Clustering centroids for inputs.
  repeated Cluster clusters = 8;
  // Embedding vectors representing each input.
  repeated Embedding embeddings = 9;
  // For recursing into localized regions of an input.
  repeated Region regions = 11;
  // For temporal content like video.
  repeated Frame frames = 12;
  // Input, output or annotation text.
  Text text = 13;
  // Input and output audio.
  Audio audio = 14;
  // Track information.
  repeated Track tracks = 15;
  // Time segments information.
  repeated TimeSegment time_segments = 16;
  // Holds score, rank, and user, app, input IDs and search hit data
  repeated Hit hits = 17;
  // Heatmap as 2d image
  repeated Image heatmaps = 18;
}

// A region within the data.
message Region {
  // A unique id for the region.
  string id = 1;
  // The details about the location of the region.
  RegionInfo region_info = 2;
  // A recursive definition of the data within the Region. For example, this will contain
  // data.concepts if the region also has annotations or predictions of concepts within it.
  Data data = 3;
  // This is the confidence score of the overall Region.
  float value = 4;
  // For tracking algorithsm and annotations we tie regions together with this track id.
  string track_id = 5;
}

// The information of the location of the Region.
message RegionInfo {
  reserved 2, 3;

  // Details of the region's rectangular bounding box.
  BoundingBox bounding_box = 1;
  // Details of the region's segmentation mask.
  Mask mask = 4;
  // A polygon of points.
  Polygon polygon = 5;
  // A landmark point location.
  Point point = 6;
  // Span char sequence for NLP.
  Span span = 7;
  // Token char sequence for NLP.
  Token token = 8;
  // The locations of detected keypoints, which are to be used in conjunction with the detected concept's skeleton to connect the keypoint locations.
  // These will be in the same order as the respective keypoint_names inside the concept.
  repeated Point keypoint_locations = 9;
}

// Rectangular bounding box for a region.
message BoundingBox {
  // The top left of the bounding box normalized to the data dimension to be within [0-1.0]
  float top_row = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The left column of the bounding box normalized to the data dimension to be within [0-1.0]
  float left_col = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The bottom row of the bounding box normalized to the data dimension to be within [0-1.0]
  float bottom_row = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The right col of the bounding box normalized to the data dimension to be within [0-1.0]
  float right_col = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
}

// The information of the location of the Frame.
message FrameInfo {
  // Deprecated. Use Time instead.
  // The index of the frame, informational and optional.
  // Depends on the sampling rate used during processing
  // May be 0 for interpolated frames that are generated for brief time (training) or if new frame is manually added
  uint32 index = 1 [(clarifai.api.utils.cl_show_if_empty) = true, deprecated = true];
  // time in the video in milliseconds. This is independent of the sampling rates used during
  // processing.
  uint32 time = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
}

// A Frame of time-series Data such as a Video.
message Frame {
  // Information aboue frame such as number and time.
  FrameInfo frame_info = 1;
  // A recursive definition of the data within the Frame. For example, this will contain
  // data.concepts if the Frame also has annotations or predictions of concepts within it.
  // This can also have data.regions for annotation or predictions of detection regions, which can
  // then recursively have their data field filled in as well.
  Data data = 2;
  // An ID for the frame.
  string id = 3;
}

// Segmentation mask.
message Mask {
  reserved 1;

  // The image of the mask in a non-raster format.
  Image image = 2;
}

// Polygon
message Polygon {
  // A list of points connected together to form the polygon.
  repeated Point points = 1;
}

// Point
message Point {
  // The row location of the point. This has a [0.0-1.0] range with 0.0 being top row and 1.0
  // being the bottom row.
  float row = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The column location of the point. This has a [0.0-1.0] range with 0.0 being left col and 1.0
  // being the right col.
  float col = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Depth if applicable for the point.
  float z = 3;
  // Whether this point is visible or occluded
  enum Visibility {
    NOT_SET = 0;     // Visibility of the point is not set
    VISIBLE = 1;     // Point is visible
    NOT_VISIBLE = 2; // Point is occluded
    NOT_PRESENT = 3; // Point is not in the image
  }
  Visibility visibility = 4;
}

message Span {
  uint32 char_start = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 char_end = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  string raw_text = 3;
}

message Token {
  uint32 char_start = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 char_end = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  string raw_text = 3;
}

// Embedding
message Embedding {
  repeated float vector = 1 [packed = true];
  uint32 num_dimensions = 2;
}

// GeoPoint
message GeoPoint {
  float longitude = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  float latitude = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
}

// GeoLimit
message GeoLimit {
  string type = 1;
  float value = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
}

// GeoBoxedPoint
message GeoBoxedPoint { GeoPoint geo_point = 1; }

// Geo
message Geo {
  GeoPoint geo_point = 1;
  GeoLimit geo_limit = 2;
  // NOTE: inconsistency: should have been geo_boxed_points
  repeated GeoBoxedPoint geo_box = 3;
}

// Image
message Image {
  reserved 3;

  // This is a URL to a publicly accessible image file. The platform will download this file server
  // side and then process.
  string url = 1;
  // The base64 field is using image file bytes directly in the request.
  // NOTE: if you're sending a json request, then this MUST be base64 encoded before sending (hence
  // the name here).
  // When using our grpc clients, you DO NOT need to base64 encode
  // it yourself since the clients know how to do this for you automatically and will avoid the
  // base64 encoding if they send a binary request.
  bytes base64 = 2;

  bool allow_duplicate_url = 4;
  // The hosted field lists images in different sizes hosted in Clarifai storage.
  HostedURL hosted = 5;
  // image info
  ImageInfo image_info = 6;
}

message ImageInfo {
  // width
  int32 width = 1;
  // height
  int32 height = 2;
  // image format
  string format = 3;
  // image color mode
  string color_mode = 4;
}

// HostedURL
message HostedURL {
  // Prefix of the URL of every hosted image.
  string prefix = 1;
  // Suffix of an image stored in different sizes.
  string suffix = 2;
  // The sizes field lists which images of the different sizes are hosted in our storage. The URL
  // of each hosted image can be obtained by joining the prefix, one of the sizes and suffix.
  repeated string sizes = 3;
  // The crossorigin property of html media tag
  // For Secure Data Hosting this needs to be set to 'use-credentials'
  string crossorigin = 4;
}

// Input
message Input {
  reserved 3;

  // The ID for the input
  string id = 1;

  // The data passed along in this input.
  Data data = 2;

  // When the input was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 4;

  // When the input was modified.
  google.protobuf.Timestamp modified_at = 5;

  // This is the status at a per Input level which allows for
  // partial failures.
  clarifai.api.status.Status status = 6;

  // List of dataset IDs that this input is part of
  // Currently, this field is ONLY used to
  // * search inputs part of dataset(s), e.g. in `PostSearches`, `PostInputsSearches` and `PostAnnotationsSearches` endpoints, and
  // * to add inputs to dataset(s) in `PostInputs` endpoint.
  // Note that this field is ignored for other endpoints, e.g. `GetInput`, `ListInputs` and `PatchInputs`.
  repeated string dataset_ids = 7;
}

// InputBatch is a batch of Input resources. Large amounts of inputs are usually
// divided into multiple InputBatches.
message InputBatch { repeated Input inputs = 1; }

// NOTE: inconsistency: this is weird mix of plural and singular words.
message InputCount {
  uint32 processed = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 to_process = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 errors = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 processing = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 reindexed = 5 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 to_reindex = 6 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 reindex_errors = 7 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 reindexing = 8 [(clarifai.api.utils.cl_show_if_empty) = true];
}

// Dataset
message Dataset {
  reserved 6, 10;

  // The ID for the dataset
  string id = 1;

  // When the dataset was created.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 2;

  // When the dataset was modified.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp modified_at = 3;

  // The app the dataset belongs to.
  string app_id = 4;

  // The user the dataset belongs to.
  string user_id = 5;

  // Description of the dataset
  string description = 7;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 8;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 9;

  // Default annotation filter used for this dataset.
  AnnotationFilter default_annotation_filter = 12;

  // Default processing info used for this dataset.
  DatasetVersionProcessingInfo default_processing_info = 16;

  // Notes for the dataset
  // This field should be used for in-depth notes and supports up to 64Kbs.
  string notes = 11;

  // Dataset version associated with this dataset. This is used in listing Datasets
  // and including the latest version.
  DatasetVersion version = 13;

  // Whether the dataset is starred by the requesting user.
  bool is_starred = 14;
  // Number of users that starred this dataset.
  int32 star_count = 15;

  // bookmark info. When set, this dataset is a bookmarked dataset of this app.
  // Info in this field will allow you to find/access original dataset.
  BookmarkOrigin bookmark_origin = 17;
  // Representative image for this dataset
  Image image = 18;
}

// AnnotationFilter is used to create a new dataset version.
// For now, the filter is simply a wrapper over a Search.
// In the future, we may add extra fields to customize the filtering.
message AnnotationFilter {
  reserved 6, 7, 8;

  // The ID for the annotation filter
  string id = 1;

  // When the annotation filter was created.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 2;

  // When the annotation filter was modified.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp modified_at = 3;

  // The user the annotation filter belongs to.
  string user_id = 4;

  // The app the annotation filter belongs to.
  string app_id = 5;

  // The search that this filter uses.
  Search search = 9;
}

// DatasetInput
message DatasetInput {
  // When the input was added to the dataset.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 1;

  // The input data.
  Input input = 2;
}

// DatasetVersion
message DatasetVersion {
  reserved 7, 9, 11;

  // The ID for the dataset version
  string id = 1;

  // When the dataset version was created.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 2;

  // When the dataset version was modified.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp modified_at = 3;

  // The app the dataset version belongs to.
  string app_id = 4;

  // The user the dataset version belongs to.
  string user_id = 5;

  // The dataset the dataset version belongs to.
  string dataset_id = 6;

  // Data config reveals how the dataset version is generated.
  oneof data_config {
    // The dataset version will be generated based on a single annotation filter.
    AnnotationFilterConfig annotation_filter_config = 15;
    // The dataset version will be generated based on model version inferences.
    ModelPredictConfig model_predict_config = 18;
  }

  // Status for this dataset version.
  clarifai.api.status.Status status = 8;

  // Description of the dataset version
  string description = 10;

  // Dataset version processing. If this is not set when the dataset version is
  // created, then the dataset default_processing_info is copied instead. Later
  // updates to default_processing_info will not apply to existing versions.
  DatasetVersionProcessingInfo processing_info = 19;

  // Dataset version metrics
  map<string, DatasetVersionMetrics> metrics = 16;

  // Dataset version exports
  DatasetVersionExportInfo export_info = 17;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 12;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 13;

  // The embedding models to return embeddings for. If empty, no embeddings are returned.
  repeated string embed_model_version_ids = 14;
}

message AnnotationFilterConfig {
  // The annotation filter that is used.
  AnnotationFilter annotation_filter = 1;

  // If true, empty inputs are not included in the dataset version.
  // If false, empty inputs are included in the dataset version.
  // We define an empty input as an input without any annotations after annotation filter is applied.
  bool ignore_empty_inputs = 2;
}

message ModelPredictConfig {
  // Assumed to be owned by the calling users app unless user_id and app_id are filled out.
  Model model = 1;
}

message DatasetVersionMetrics {
  reserved 2, 3, 4, 5, 7;

  // Number of inputs
  google.protobuf.UInt64Value inputs_count = 1;
  // Number of unlabeled inputs
  // An input is considered unlabeled if it there are no annotations with positive labels for that input.
  google.protobuf.UInt64Value unlabeled_inputs_count = 6;
  // Number of inputs that have metadata
  google.protobuf.UInt64Value inputs_with_metadata_count = 8;
  // Number of inputs that have geo information
  google.protobuf.UInt64Value inputs_with_geo_count = 9;

  // Number of regions
  google.protobuf.UInt64Value regions_count = 20;
  // The matrix shows where the regions are located.
  // Example: If the matrix has 2x2 dimensions, then
  // * region_location_matrix[0][0] = the number of regions that appear in the top left corner, i.e. [0,0]..(0.5,0.5)
  // * region_location_matrix[0][1] = the number of regions that appear in the top right corner, i.e. [0,0.5]..[0.5,1]
  // * region_location_matrix[1][0] = the number of regions that appear in the bottom left corner, i.e. [0.5,0]..[1,0.5)
  // * region_location_matrix[1][1] = the number of regions that appear in the bottom right corner, i.e. [0.5,0.5]..[1,1]
  MatrixUint64 region_location_matrix = 21;
  // Number of bounding boxes
  google.protobuf.UInt64Value bounding_boxes_count = 22;
  // Number of polygons
  google.protobuf.UInt64Value polygons_count = 23;
  // Number of points
  google.protobuf.UInt64Value points_count = 24;
  // Number of masks
  google.protobuf.UInt64Value masks_count = 25;

  // Number of inputs that have regions attached
  // Note that this is not a recursive count: if an input contains frames that contains regions, then the region_frames_count is increased, but region_inputs_count is not increased.
  google.protobuf.UInt64Value region_inputs_count = 60;
  // Number of frames that have regions attached
  google.protobuf.UInt64Value region_frames_count = 61;

  // Number of frames
  google.protobuf.UInt64Value frames_count = 30;

  // Number of inputs that have frames attached
  google.protobuf.UInt64Value frame_inputs_count = 70;

  // Number of embeddings
  google.protobuf.UInt64Value embeddings_count = 40;

  // Number of positive tags added at input-level
  google.protobuf.UInt64Value positive_input_tags_count = 50;
  // Number of positive tags added at region-level
  google.protobuf.UInt64Value positive_region_tags_count = 51;
  // Number of positive tags added at frame-level
  google.protobuf.UInt64Value positive_frame_tags_count = 52;
}

message DatasetVersionMetricsGroup {
  string parent_path = 1;
  DatasetVersionMetricsGroupType type = 2;
  google.protobuf.Value value = 3;
  DatasetVersionMetrics metrics = 4;
}
enum DatasetVersionMetricsGroupType {
  DATASET_VERSION_METRICS_GROUP_TYPE_NOT_SET = 0;

  // Group data examples by input type.
  // Examples: images, videos, text, audio.
  INPUT_TYPE = 2;
  // Group data examples by concept ID.
  // Examples: inputs with cat concept, inputs with dog concept.
  CONCEPT_ID = 10;
  // Group data examples by concepts count.
  // Examples: inputs with 20 concepts, inputs with 21 concepts.
  CONCEPTS_COUNT = 11;
  // Group data examples by bounding boxes count.
  // Examples: inputs with 20 bounding boxes, inputs with 21 bounding boxes.
  BOUNDING_BOXES_COUNT = 20;
  // Group data examples by polygons count.
  // Examples: inputs with 20 polygons, inputs with 21 polygons.
  POLYGONS_COUNT = 21;
  // Group data examples by points count.
  // Examples: inputs with 20 points, inputs with 21 points.
  POINTS_COUNT = 22;
  // Group data examples by masks count.
  // Examples: inputs with 20 masks, inputs with 21 masks.
  MASKS_COUNT = 23;
  // Group data examples by pixels count.
  // In order to reduce the number of groups, we use bins.
  // Examples for bin size = 400: inputs with [200000, 200400) pixels, inputs with [200400, 200800) pixels.
  PIXELS_COUNT = 30;
  // Group data examples by aspect ratio.
  // In order to reduce the number of groups, we use bins.
  // Examples for bin size = 0.1: inputs with [0.5, 0.6) aspect ratio, inputs with [0.6, 0.7) aspect ratio.
  ASPECT_RATIO = 31;
}


// DatasetVersionExportInfo contains information about all exports of a dataset version.
//
// If the dataset version has not been exported in a format, then the DatasetVersionExport
// field for that format is empty instead of having a "not exported" status.
message DatasetVersionExportInfo {
  // clarifai_data_protobuf is a CLARIFAI_DATA_PROTOBUF export of the dataset version.
  DatasetVersionExport clarifai_data_protobuf = 1;

  // clarifai_data_json is a CLARIFAI_DATA_JSON export of the dataset version.
  DatasetVersionExport clarifai_data_json = 3;

  // coco is a COCO export of the dataset version.
  DatasetVersionExport coco = 2;
}

// DatasetVersionExport contains metadata for a single dataset version export.
message DatasetVersionExport {
  // format is the format of the dataset version export.
  DatasetVersionExportFormat format = 1;

  // status is the current status of the dataset version export.
  clarifai.api.status.Status status = 2;

  // url is the URL from where the dataset version export can be downloaded.
  string url = 3;

  // size is the size of the dataset version export in number of bytes.
  uint64 size = 4;

  // whether to include embeddings in the export or not.
  bool include_embeddings = 5;
}
enum DatasetVersionExportFormat {
  DATASET_VERSION_EXPORT_FORMAT_NOT_SET = 0;

  // CLARIFAI_DATA_PROTOBUF is the proprietary Clarifai API Data format. It
  // is a ZIP-archive containing batches of serialized InputBatch protobuf messages.
  //
  // Note that only the "id" and "data" fields of exported inputs are set.
  CLARIFAI_DATA_PROTOBUF = 1;

  // CLARIFAI_DATA_JSON is the proprietary Clarifai API Data format in JSON. It
  // is a ZIP-archive containing batches of serialized InputBatch JSON messages.
  //
  // Note that only the "id" and "data" fields of exported inputs are set.
  CLARIFAI_DATA_JSON = 3;

  // COCO is the data format used by Common Objects in Context. It is a
  // ZIP-archive containing JSON files with the dataset version annotations.
  // See https://cocodataset.org/#format-data.
  COCO = 2;
}


// DatasetVersionProcessingInfo contains information about processing applied
// to a dataset version.
message DatasetVersionProcessingInfo {
  // If frame_interpolation_info is set, then these settings are used to
  // interpolate new frame annotation from other video annotations.
  //
  // If frame_interpolation_info is set in the dataset default_processing_info,
  // then it can be disabled for a single dataset version by setting
  // processing_info but not setting processing_info.frame_interpolation_info.
  FrameInterpolationInfo frame_interpolation_info = 1;
}

// FrameInterpolationInfo contains information about frame annotations
// interpolated from other video annotations, such as image object-detection
// regions generated from video object-tracking regions.
message FrameInterpolationInfo {
  // sample_ms is the sampling rate at which frame annotations are interpolated.
  // If sample_ms is zero, then the dataset default_processing_info value is used.
  // If the dataset default is zero or not set, then the input frame prediction
  // sampling rate is used.
  uint32 sample_ms = 1;
}

// WorkflowResultsSimilarity
message WorkflowResultsSimilarity {
  // The input with the specific data compare against all pool results
  Input probe_input = 1;
  repeated Hit pool_results = 2;
}

// Key
message Key {
  // The id of this key, it is used for authorization.
  string id = 1;
  // The type of key, it can be api_key or personal_access_token, the default value is api_key
  string type = 8;
  // The description
  string description = 2;
  // The low-level scopes this key has
  repeated string scopes = 3;
  // The endpoint-level scopes this key has
  repeated string endpoints = 7;
  // The apps that this key give you access to, it is empty if this key is personal_access_token
  // API key can only give you access to a single app.
  repeated App apps = 4;

  // When the key was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 5;

  // When does the key expires, the key won't expire if this is empty
  google.protobuf.Timestamp expires_at = 6;

  // list of idp ids at which key is currently authorized
  repeated string authorized_idp_ids = 9;
}


enum ExpirationAction {
  EXPIRATION_ACTION_NOT_SET = 0;

  DELAY = 1;  // Progressively delay the execution of operations
  EXPIRY = 2; // Cease functioning
}

enum LicenseScope {
  LICENSE_SCOPE_NOT_SET = 0;

  PREDICT = 1;
  TRAIN = 2;
  SEARCH = 3;
}


// This is the Model object which represents a created model in the platform.
// Each model has a particular type denoted by the model_type_id.
// When creating a Model with PostModels the following happens:
//  - if the ModelType is trainable, then a new ModelVersion is created that is
//    - UNTRAINED status by default
//    - TRAINED status if a ModelVersion was included with PretrainedModelConfig in PostModels
//  - if the ModelType is not trainable, then a new ModelVersion is created with TRAINED status.
// To modify config settings like OutputInfo for the Model you an use PatchModels. This will
// also create a new ModelVersion, potentially UNTRAINED following the same rules as above.
// The fields that are patchable include Model.name, Model.display_name and Model.output_info
// (except the Model.output_info.type and Model.output_info.type_ext).
message Model {
  reserved 8, 10, 11, 12, 13, 24, 28;

  // The model's ID. Must be unique within a particular app and URL-friendly.
  string id = 1;
  // DEPRECATED: Please use the model id to name the model.
  string name = 2 [deprecated = true];
  // When the model was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  //  the following from the API:
  //  "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 3;
  // When was the most recent model version created at
  google.protobuf.Timestamp modified_at = 19;
  // The app the model belongs to.
  string app_id = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Info about the model's output and configuration.
  // DEPRECATED: Will be moved to model version
  OutputInfo output_info = 5 [deprecated = true];
  // A particular version of the model, e.g., to specify the version when creating a workflow or
  // when listing Models to include the latest ModelVersion of the model in the response.
  ModelVersion model_version = 6;
  // DEPRECATED: Please use the model id to name the model.
  string display_name = 7 [deprecated = true];
  // The user id that the model belongs to.
  string user_id = 9;

  // The default evaluation info. Can be overwritten by eval request.
  EvalInfo default_eval_info = 30;
  // The ModelType.Id that is used for this model. This is used for all versions and you cannot
  // change model_type_id between versions of the same model.
  string model_type_id = 14;
  // The task the model was trained to do
  string task = 26;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 15;

  // Short description about this model
  string description = 16;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 17;
  google.protobuf.Struct presets = 27;

  // Notes for the model
  // This field should be used for in-depth notes and supports up to 64Kbs.
  string notes = 18;

  // Tags from toolkits category
  repeated string toolkits = 20 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Tags from use_cases category
  repeated string use_cases = 21 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Tags from languages category.
  repeated string languages = 25 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Tags from languages category with names, only used in responses.
  repeated FullTag languages_full = 31 [(clarifai.api.utils.cl_show_if_empty) = true];

  repeated string check_consents = 32 [(clarifai.api.utils.cl_show_if_empty) = true];

  // Is starred by the requesting user (only showed on get/list requests)
  // Please use PostModelStars/DeleteModelStars endpoints to star/unstar a model
  bool is_starred = 22;
  // How many users have starred the model (only showed on get/list requests)
  // Computed value, not editable
  int32 star_count = 23;

  // Whether it's recommended that this model is used within a workflow
  google.protobuf.BoolValue workflow_recommended = 29;

  // bookmark info. When set, this model is a bookmarked model of this app.
  // Info in this field will allow you to find/access original model.
  BookmarkOrigin bookmark_origin = 33;
  // Representative image for this model
  Image image = 34;
}

// A link to a html/markdown/text file that stores reference material tied to a model.
message ModelReference {
  // Id of the reference
  string id = 1;

  // The id of the model this Model reference is tied to.
  string model_id = 2;

  // address of resource
  string url = 3;

  // name of link
  string name = 4;

  // To handle arbitrary json metadata:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 5;
}

// ModelVersionInputExample
message ModelVersionInputExample {
  // user unique id
  string id = 1;
  // external id of model
  string model_id = 2;
  // external id of model version
  string model_version_id = 3;
  // data to store as example input for model
  Data data = 4;
  // name of link for display
  string name = 5;
  // description of link contents
  string description = 6;
}

// OutputInfo defines some of the settings for each model version that PatchModels can effect. These
// parameters control some of the training or inference operations that this model can do.
// As the number of parameters continued to grow when we launched more ModelTypes we decided to move
// to using the OutputInfo.params field which is a Struct (or JSON object if you're using
// our JSON REST APIs). This allows each ModelType to define the set of fields, their default values
// and description of each field so that we can display those in Portal and make the creation of
// Model's very extensible. The OutputConfig object will eventually go away in favor of
// infer_params struct.
message OutputInfo {
  reserved 4, 5;
  // List of concepts or other output related data for the model.
  Data data = 1;
  // Model configuration...going away in favor of infer_params and train_params over time.
  // TO BE DEPRECATED
  OutputConfig output_config = 2;
  // For returning where to look for the Output info if not returning it.
  string message = 3;
  // Map from the api.Data field names to the underlying model graph's outputs. When using a
  // PretrainedModelConfig the values in this map need to match the Triton config.pbtxt output names.
  google.protobuf.Struct fields_map = 6;

  // For predicting with the various ModelType's we accept a Struct (JSON object) worth of args
  // that the ModelTypeField defines. During inference, the settings contained within are sent
  // to the model predictor to alter predictions from this Model.
  google.protobuf.Struct params = 7;
  repeated ModelTypeField params_specs = 8;
}

// InputInfo
message InputInfo {
  // Map from the api.Data field names to the underlying model graph's inputs. When using a
  // PretrainedModelConfig the values in this map need to match the Triton config.pbtxt input names.
  google.protobuf.Struct fields_map = 1;

  // To control the inputs to the given model we allow a list of parameters
  // defined for each ModelType as a Struct (JSON object) here. During training or inference, the
  // settings contained within are sent to the training processor to alter the training process.
  google.protobuf.Struct params = 2;
  // For base model to get embeddings from for transfer learned models.
  Model base_embed_model = 3;
}

message TrainInfo {
  // To control the training process when PostModelVersions is used we allow a list of parameters
  // defined for each ModelType as a Struct (JSON object) here. During training, the settings
  // contained within are sent to the training processor to alter the training process.
  google.protobuf.Struct params = 1;
  // The dataset and dataset version this model version was or will be trained on
  Dataset dataset = 2;
}

message EvalInfo {
  // To control the evaluation process.
  // Allow a list of parameters.
  google.protobuf.Struct params = 1;
}

message ImportInfo {
  // Used to configure model imports from third-party toolkits.
  google.protobuf.Struct params = 1;
}

// OutputConfig is a collection of parameters controlling either inference or training settings for
// the given Model. This message will be deprecated over time in favor or infer_params and
// train_params in OutputInfo which are cleaner and more extensible for many ModelTypes.
message OutputConfig {
  reserved 11, 12, 16, 18;

  // For custom concept model training: whether the concept predictions must sum to 1.
  bool concepts_mutually_exclusive = 1 [deprecated = true];
  // DEPRECATED: For custom models, this is the base model to use for image embeddings.
  // Default is general model.
  string existing_model_id = 3 [deprecated = true];
  // For concept model predictions: Overrides the default_language for the app in a predict call.
  string language = 4;
  // DEPRECATED: Hyper-parameters for custom training.
  // Use new hyper_params field instead.
  string hyper_parameters = 5 [deprecated = true];
  // For concept model predictions:  Maximum number of concepts in result. Defaults to 0 which under
  // the hood will return default of 20. We do a server side default in order to control this
  // feature in the future.
  uint32 max_concepts = 6 [(clarifai.api.utils.cl_show_if_empty) = true];
  // For concept model predictions: Minimum value of concept's probability score in result.
  // Defaults to 0.0 which means we won't do any thresholding as all probabilities will
  // likely be > 0.0.
  float min_value = 7 [(clarifai.api.utils.cl_show_if_empty) = true];
  // For concept model predictions: Select concepts in result by name or by id
  repeated Concept select_concepts = 8;
  // For custom concept model training: Training timeout of the model (in seconds)
  uint32 training_timeout = 9;
  // For model predictions on video: Sample delay for video predicting (1 frame per N milliseconds)
  uint32 sample_ms = 10;
  // For custom model training: Hyperparameters for custom training
  google.protobuf.Struct hyper_params = 13;
  // For custom model training: this is the base model version to use for image embeddings.
  // This has to be one of the embed models in the app workflow.
  string embed_model_version_id = 14 [deprecated = true]; // Use input_info.base_embed_model instead.
  // For custom model training: Use this flag to fail on missing positive examples
  // By default we fill in the missing with random examples
  bool fail_on_missing_positive_examples = 15;
  // For custom model training: This is any additional metadata as a JSON object that we want
  // want to persist in the model's output config. This is a useful quick way to set fields for
  // introducing fields for new model types so we don't have to add a new proto field and DB field
  // each time. Please refer to the documentation or model implementation internally for more
  // details on what fields are supported for which models.
  // TODO(zeiler): remove this field after Portal is updated.
  google.protobuf.Struct model_metadata = 17 [deprecated = true];
}

// ModelSpec is a definition of a Model type. This is used in model mode of portal
// to list out the possible models that can be created and can be used to understand more about
// the possible models in our platform.
message ModelType {
  reserved 7, 4, 13, 14, 15;
  // A unique identifies for this model type. This is differnt than the 'type' field below because
  // the 'type' can be re-used for differnet input and output combinations whereas 'id' is always
  // unique.
  string id = 1;
  // title for this model in model gallery
  string title = 2;
  // Description of this model type.
  string description = 3;
  // The list of input fields that this model accepts. These are the keys of the Model's
  // InputInfo.fields_map
  repeated string input_fields = 5;
  // The list of output fields that this model accepts. These are the keys of the Model's
  // OutputInfo.fields_map
  repeated string output_fields = 6;
  // Is this model trainable in our platform.
  bool trainable = 8;
  // Is this model creatable. We have some pre-trained model types that users cannot create yet in
  // model mode.
  bool creatable = 9;
  // Is this model type only for internal users at this time.
  bool internal_only = 10;

  // The remaining fields are definitions of the configurable fields that exist.
  // Each field has path into the Model object such as "name" as a top level or "output_info.data"
  // if it's the Data object within the OutputInfo object. We decided to not break these up
  // into input_info, train_info and output_info related parameters and instead use the path
  // so that they are most flexible.
  repeated ModelTypeField model_type_fields = 11;

  // For sequence models we need to know when processing that they require temporal time frames
  // in sequential order. This will be true for model types like trackers as an example.
  bool requires_sequential_frames = 12;

  // Expected input layers of an uploaded model
  repeated ModelLayerInfo expected_input_layers = 16;

  // Expected output layers of an uploaded model
  repeated ModelLayerInfo expected_output_layers = 17;

  EvaluationType evaluation_type = 18;
}

message ModelLayerInfo {
  // The api.Data field this layer will be parsed into
  string data_field_name = 1;
  // Description of the expected shape. Can support multiple support layer shapes.
  repeated LayerShape shapes = 2;
  // Brief description about the layer if needed
  string description = 3;
  // Whether this layer should have a label_filename specified and provided
  bool requires_label_filename = 4;
}

message TritonCondaEnvInfo {
  string conda_pack_url = 1;
  string conda_yaml_url = 2;
}
enum DataType {
  UNDEFINED = 0; // Default value, should not be used
  STRING = 1;
  UINT8 = 2;
  INT32 = 3;
  INT64 = 4;
  FP32 = 5;
}


message LayerShape {
  // Supported dimensions
  // Example: [-1,4] is a 2-dimensional array with the first dimension of variablesize, but second dimension with a static size: [[1,2,3,4],[4,5,6,7],...]
  repeated int32 dims = 1;
  // Max dimension size, applicable to layers that can have flexible sizes.
  repeated int32 max_dims = 2;
  // The triton data type
  DataType data_type = 3;
  // Description about the dimensions
  string description = 4;
}

// ModelTypeField stores a field value of a configurable type.
message ModelTypeField {
  // The path where the value of the field will be stored.
  // Example:
  // "output_info.data" would be the Data message in the OutputInfo message.
  // "output_info.output_config.language" is in the OutputConfig message within OutputInfo
  // "input_info.params" is in the params struct within InputInfo.
  // "output_info.params" is in the params struct within OutputInfo.
  // "train_info.params" is in the params struct within TrainInfo.
  // and so on.
  string path = 1;
  // These are various types of fields that we have UIs for.
  enum ModelTypeFieldType {
    reserved 6;

    INVALID_MODEL_TYPE_FIELD_TYPE = 0;

    BOOLEAN = 1;
    STRING = 2;
    NUMBER = 3;
    // For auto-completing to concepts in the app. This goes into an data.concepts field.
    ARRAY_OF_CONCEPTS = 4;
    // For auto-completing to concepts in the app. This goes into an data.concepts field.
    ARRAY_OF_CONCEPTS_WITH_THRESHOLD = 5;
    // A range for a float value.
    RANGE = 7;
    // If ENUM is used then the "enum_options" field should also be filled in with the respective ID and description
    // for the different ENUM options.
    ENUM = 8;
    // For listing collaborators of the app. The field is a string of the collaborator's user_id.
    COLLABORATORS = 9;
    // For arbitrary json object: "{...}"
    JSON = 10;
    // Such as [1.0, 2.0, 3.5]
    ARRAY_OF_NUMBERS = 11;
    // For selecting the embed_model_version_id for context based models.
    // This is a string type in the API request.
    WORKFLOW_EMBED_MODELS = 12;
    // Such as ['a', 'b', 'cantaloupe']
    ARRAY_OF_STRINGS = 13;
    // If RECURSIVE_ENUM is used then the "enum_options" field should also be filled in with the respective ID and
    // description for the different RECURSIVE_ENUM options, as well as model_type_fields for each enum choice.
    RECURSIVE_ENUM = 14;
    // For blocks of code that need to be specified by the user for setup or execution during workflow runs.
    PYTHON_CODE = 15;
    // For selecting a dataset id in model parameters. String in API request.
    DATASET_ID = 16;
    // For selecting a dataset version id. String.
    DATASET_VERSION_ID = 17;
    // For auto-completing to concepts in the model.
    ARRAY_OF_MODEL_CONCEPTS = 18;
    // For selecting a dataset
    DATASET = 19;
    // For selecting a dataset version
    DATASET_VERSION = 20;
    // To pass a string downstream, that is encrypted in the DB and API.
    ENCRYPTED_STRING = 21;
  }
  // The field for this field.
  ModelTypeFieldType field_type = 2;
  // A default value. We use the Value field because we want to have structured data (just like
  // google.protobuf.Struct but this is just a single value).
  google.protobuf.Value default_value = 3;
  // Description for this field.
  string description = 4;
  // Placeholder text for the UI element.
  string placeholder = 5;
  // List of options of the ENUM type and potentially additional fields they bring with them.
  repeated ModelTypeEnumOption model_type_enum_options = 6;
  // If this field should appear for internal users only.
  bool internal_only = 7;
  // If this field is a required field. If True then during validation you won't be able to create
  // a model of this type with providing a value for this field. When False, the ModelType's
  // default_value will be used for this field.
  bool required = 8;
  // If the field_type is RANGE, this must be filled in.
  ModelTypeRangeInfo model_type_range_info = 9;
}

// ModelTypeRangeInfo
message ModelTypeRangeInfo {
  // The start of the range as a float.
  float min = 1;
  // The end of the range as a float.
  float max = 2;
  // An optional step size for the range. If provided then only values at that step size will be
  // rounded to. For example if step is 0.02 then 0.0245 will round to 0.02.
  float step = 3;
}

// ModelTypeEnumOption
message ModelTypeEnumOption {
  // The unique value of the enum option.
  string id = 1;

  // List of other ID values that are equivalent with this ID.
  // This allows the user to choose this option by multiple IDs.
  // Example: if enum is "Phone Number Prefix", you could add an option that is selectable by two values:
  // 1. ID: "Estonia"
  // 2. Alias: 37
  repeated ModelTypeEnumOptionAlias aliases = 5;

  // Optional description for this enum option.
  string description = 2;
  // These are additional fields that are specific to this enum choice. This allows
  // us to use enums to control configuration settings as well.
  repeated ModelTypeField model_type_fields = 3;

  // If this enum option should be internal only.
  bool internal_only = 4;

  // Whether this is the recommended enum option. Set to `true` when there
  // are multiple options, and one is shown to be better than the others.
  bool recommended = 6;
}

message ModelTypeEnumOptionAlias {
  // Integer alias for id.
  int64 id_int = 1;
  // String that can contain wild cards and the regex needs to match.
  string wildcard_string = 2;
}

// ModelQuery
message ModelQuery {
  reserved 2;
  // The name ofthe field. This supports wilcard queries like "gen*" to match "general" as an example.
  string name = 1;
  // Filter models by the specific model_type_id. See ListModelTypes for the list of ModelType.Id's
  // supported.
  string model_type_id = 3;
}
enum ValueComparator {
  CONCEPT_THRESHOLD_NOT_SET = 0;

  // input > value
  GREATER_THAN = 1;
  // input >= value
  GREATER_THAN_OR_EQUAL = 2;
  // input < value
  LESS_THAN = 3;
  // input <= value
  LESS_THAN_OR_EQUAL = 4;
  // input == value
  EQUAL = 5;
}

enum EvaluationType {
  Undefined = 0;
  Classification = 1; // default
  Detection = 2;
  Segmentation = 3;
  Clustering = 4;
  Tracker = 5;
}


// ModelVersion
message ModelVersion {
  reserved 9, 18;

  string id = 1;
  // When the version was created.
  google.protobuf.Timestamp created_at = 2;
  // The status of the version (whether it's untrained, training, trained, etc.).
  clarifai.api.status.Status status = 3;

  uint32 active_concept_count = 4;

  EvalMetrics metrics = 5;

  // number of inputs in the model version
  uint32 total_input_count = 6;

  PretrainedModelConfig pretrained_model_config = 7;

  // Detailed training stats.

  // When training of this version was completed.
  google.protobuf.Timestamp completed_at = 10;

  // Description about this version
  string description = 11;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 12;

  // The app the model version belongs to.
  string app_id = 13;
  // The user the model version belongs to.
  string user_id = 14;

  // When this model version was last modified
  google.protobuf.Timestamp modified_at = 15;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 16;

  string license = 17;

  // Info about the model's output and configuration.
  OutputInfo output_info = 19;
  // Info about the models' input and configuration of them.
  InputInfo input_info = 20;
  // Configuration for the training process of this model.
  TrainInfo train_info = 21;
  // Configuration used to import model from third-party toolkits
  ImportInfo import_info = 22;
  // Contains the training logs if available
  string train_log = 23;
}

// PretrainedModelConfig
message PretrainedModelConfig {
  reserved 2, 5;
  // This is the internal id of the pretrained model.
  // Map from the api.Data field names to the Triton config.pbtxt input.
  google.protobuf.Struct input_fields_map = 3;
  // Map from the api.Data field names to the Triton config.pbtxt output.
  google.protobuf.Struct output_fields_map = 4;
  // Url to a zipped up model in triton format with the following files and folders at the root:
  //  config.pbtxt
  //  version 1 folder that contains model files (onnx graph, torch script, python BE model, and etc.)
  string model_zip_url = 6;
  // Whether to overwrite the model for the existing internal id
}

// TrainStats
message TrainStats { repeated LossCurveEntry loss_curve = 1; }

// LossCurveEntry
message LossCurveEntry {
  // current epoch
  uint32 epoch = 1;
  // current global step
  uint32 global_step = 2;
  // current cost
  // FIXME(rigel): this should be loss instead of cost.
  float cost = 3;
}

// LabelCount
message LabelCount {
  string concept_name = 1 [deprecated = true];
  uint32 count = 2;
  Concept concept = 3;
}

// LabelDistribution
message LabelDistribution { repeated LabelCount positive_label_counts = 1; }

// NOTE: this is inefficient, should just have the order of the rows/cols
message CooccurrenceMatrixEntry {
  // concept_id for the row
  string row = 1;
  // concept_id for the col
  string col = 2;
  uint32 count = 3;
}

// CooccurrenceMatrix
message CooccurrenceMatrix {
  repeated CooccurrenceMatrixEntry matrix = 1;
  // These concept_ids are ordered by the strength of the diagonal in the ConfusionMatrix.
  repeated string concept_ids = 2;
}

// ConfusionMatrixEntry
message ConfusionMatrixEntry {
  string predicted = 1;
  string actual = 2;
  float value = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
  Concept predicted_concept = 5;
  Concept actual_concept = 6;
}

// ConfusionMatrix
message ConfusionMatrix {
  repeated ConfusionMatrixEntry matrix = 1;
  // These concept_ids are ordered by the strength of the diagonal in the ConfusionMatrix.
  repeated string concept_ids = 2;
}

// ROC
message ROC {
  repeated float fpr = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  repeated float tpr = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  repeated float thresholds = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  repeated float fpr_per_image = 4;
  repeated float fpr_per_object = 5;
}

// PrecisionRecallCurve
message PrecisionRecallCurve {
  repeated float recall = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  repeated float precision = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  repeated float thresholds = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
}

// BinaryMetrics
message BinaryMetrics {
  uint32 num_pos = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 num_neg = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 num_tot = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  float roc_auc = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
  float f1 = 5 [(clarifai.api.utils.cl_show_if_empty) = true];
  Concept concept = 6;
  ROC roc_curve = 7;
  PrecisionRecallCurve precision_recall_curve = 8;
  float avg_precision = 9;
  string area_name = 10;
  double area_min = 11;
  double area_max = 12;
  float iou = 13;
}

// TrackerMetrics
message TrackerMetrics {
  // Multiple object tracking accuracy
  float mot_mota = 1;
  // Number of switches between tracks
  int32 mot_num_switches = 2;
  // MORSE fragmentation rate (a.k.a unique switch rate, only calculated in public sector)
  float morse_frag = 3;
  // Average precision calculated from all processed frames
  float avg_precision = 4;
  // The concept that we are evaluating the tracker
  string aiid = 5;
  // Same as morse_frag but calculated using MOT mapping/metrics
  float unique_switch_rate = 6;
}

// EvalTestSetEntry
message EvalTestSetEntry {
  reserved 1, 2;
  Input input = 6; // the input information

  repeated Concept predicted_concepts = 3;
  // All the ground truth concepts will be show on the top level
  repeated Concept ground_truth_concepts = 4;
  // Only region-based/frame-based app contains this annotation
  // Each annotation only contains one region
  // And the concepts is in ground_truth_concepts instead of this annotation
  Annotation annotation = 5;
}

// LOPQEvalResult
message LOPQEvalResult {
  // Rank k for which all metrics are reported.
  int32 k = 1;

  // Recall @ k assuming the brute force search is the ground truth.
  float recall_vs_brute_force = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Kendall's tau correlation @ k assuming the brute force search is the ground truth.
  float kendall_tau_vs_brute_force = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  // The percentage of the most frequent code in the indexed part of evaluation data.
  float most_frequent_code_percent = 4 [(clarifai.api.utils.cl_show_if_empty) = true];

  // Normalized Discounted Cumulative Gain (NDCG) @ k with a ground truth inferred from annotations
  // and/or prediction for this evaluation LOPQ model.
  // NDCG uses individual relevance scores of each returned image to evaluate the usefulness, or
  // gain, of a document based on its position in the result list. The premise of DCG is that
  // highly relevant documents appearing lower in a search result list should be penalized as the
  // graded relevance value is reduced logarithmically proportional to the position of the result.
  // See: https://en.wikipedia.org/wiki/Information_retrieval#Discounted_cumulative_gain
  //
  // To compute the relevance score between two images we consider two cases:
  // 1) Only one label for each image
  // An image is relevant to an image query iff they are labeled the same (score 1), and
  // not relevant otherwise (score 0)
  // 2) Multiple labels for each image
  // Here an image relevancy with respect to a single image query is measured by f-beta score
  // assuming the query image list of labels as ground truth and comparing them with that of
  // the search result. These labels can come from image annotations or if substitute_annotation_misses
  // is set, predictions of base classifier where any prediction with prob < prob_threshold are
  // discarded. To quantify the relevancy score of a single search result we opt to compute precision
  // and recall @ k for simplicity, and combine them with f-beta score to obtain a single number.
  float lopq_ndcg = 5 [(clarifai.api.utils.cl_show_if_empty) = true];
  // Brute force NDCG which gives a baseline to compare to and is a measure of how good
  // the embeddings are.
  float brute_force_ndcg = 6 [(clarifai.api.utils.cl_show_if_empty) = true];
}

// MetricsSummary
message MetricsSummary {
  float top1_accuracy = 1 [deprecated = true];
  float top5_accuracy = 2 [deprecated = true];
  float macro_avg_roc_auc = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  float macro_std_roc_auc = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
  float macro_avg_f1_score = 5 [(clarifai.api.utils.cl_show_if_empty) = true];
  float macro_std_f1_score = 6 [(clarifai.api.utils.cl_show_if_empty) = true];
  float macro_avg_precision = 7 [(clarifai.api.utils.cl_show_if_empty) = true];
  float macro_avg_recall = 8 [(clarifai.api.utils.cl_show_if_empty) = true];
  float mean_avg_precision_iou_50 = 10;
  float mean_avg_precision_iou_range = 11;

  repeated LOPQEvalResult lopq_metrics = 9;
}

// EvalMetrics
message EvalMetrics {
  clarifai.api.status.Status status = 1;
  // user id that owns this evaluation
  string user_id = 15;
  // app id that owns this evaluation
  string app_id = 16;
  // Id of this evaluation
  string id = 10;
  // Model to evaluate
  Model model = 13;
  // The ground truth dataset
  Dataset ground_truth_dataset = 14;

  MetricsSummary summary = 2;
  ConfusionMatrix confusion_matrix = 3;
  CooccurrenceMatrix cooccurrence_matrix = 4;
  LabelDistribution label_counts = 5;
  repeated BinaryMetrics binary_metrics = 6;
  repeated EvalTestSetEntry test_set = 7;
  repeated BinaryMetrics metrics_by_area = 8;
  repeated BinaryMetrics metrics_by_class = 9;
  repeated TrackerMetrics tracker_metrics = 11;

  // Evaluation parameters to pass. Expected to match what
  // is defined in the model type for the respective model.
  EvalInfo eval_info = 12;
  ExtendedMetrics extended_metrics = 17;
}

message ExtendedMetrics {
  google.protobuf.Struct user_metrics = 1;
}

// FieldsValue
message FieldsValue {
  bool confusion_matrix = 1;
  bool cooccurrence_matrix = 2;
  bool label_counts = 3;
  bool binary_metrics = 4;
  bool test_set = 5;
  bool metrics_by_area = 6;
  bool metrics_by_class = 7;
}

// Output
message Output {
  // One of these outputs per Input
  string id = 1;
  clarifai.api.status.Status status = 2;

  // When the object was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 3;

  // The model that created this Output.
  Model model = 4;
  // The input that was passed to the model to create this Output. For example if we have an image
  // model then it will take as input here an Input object with Image filled in.
  Input input = 5;
  // The output data for this Output. For example if we have a concept model then the predicted
  // concepts will appear here.
  Data data = 6;
}

// ScopeDeps
message ScopeDeps {
  // The scope
  string scope = 1;
  // Other scopes that are required.
  repeated string depending_scopes = 2;
}

// EndpointDeps
message EndpointDeps {
  // The fully qualified endpoint to
  string endpoint = 1;
  // Other scopes that are required.
  repeated string depending_scopes = 2;
}

// Hit
message Hit {
  // This is the score for the ranked Hit results of the search query. This score is a number
  // between 0.0 and 1.0 as it represents a confidence in the search Hit. For example, if you search
  // for "car" and get a close matching Hit, the score should be close to 1.0. If you get a score
  // of close to 0.0 that means it's very disimilar to your query, in this case NOT a "car". There
  // is a special intermediate score of 0.5 that means that the Hit is not really correlated with
  // your search query (ie. not similar or dissimlar to the query) which is a common occurrence
  // when using negate queries.
  // Note: some queries that are just filtering down your app of inputs may just return a score of
  // 1.0 for all Hits.
  float score = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  // This is the matched input returned from the search query. This will contain information about
  // the Input such as the url, created_at time and trusted annotation information (for backwards
  // compatibility with apps that existed before Annotations were introduced.
  Input input = 2;
  // We also provide back the specific matched annotation for the above input. We do this in order
  // to support more complex Annotation queries in the And message below. For example if we match
  // the search results to a region in your input, or a frame in a video input, this annotation
  // field will be that matched annotation info and the input will be the image/video that the user
  // originally added which contains those regions / frames.
  Annotation annotation = 3;
  // The customer-facing id of the user who owns the app the asset came from.
  string user_id = 4;
  // The cfid of the app the asset came from.
  string app_id = 5;
}

message HitCount {
  // The estimated total number of hits for the search query, not just the current page.
  uint64 estimated_total = 1;
}

// This is the common building block of a query which is a sequence of And messages ANDed together.
// Note that some fields are used too RANK results (affect the scores) and some are used to FILTER
// results (unordered subset of your app's contents). In general, FILTER operations are more
// efficient queries at scale and when combined with RANK operations can speed up search performance
// as you effectively operate on a smaller sub-set of your entire app.
message And {
  // FILTER by input.data... information.
  // This can include human provided concepts, geo location info, metadata, etc.
  // This is effectively searching over only the trusted annotation attached to an input in your
  // app. To search by more specific annotation fields use the Annotation object here.
  // ########## Supported fields ##########
  //  - data.concepts[].id
  //  - data.concepts[].name
  //  - data.concepts[].value
  //  - data.geo.geo_box[].geo_point.latitude
  //  - data.geo.geo_box[].geo_point.longitude
  //  - data.geo.geo_limit.type
  //  - data.geo.geo_limit.value
  //  - data.geo.geo_point.latitude
  //  - data.geo.geo_point.longitude
  //  - data.image.url
  //  - data.metadata - allow search with empty metadata
  //    note that searching by empty metadata will actually not influence the search results.
  //    however, in order to be user-friendly, we are still supporting searching by empty metadata.
  //  - data.metadata.fields - filter by metadata. metadata key&value fields are OR-ed.
  //  - dataset_ids[] - filter by dataset IDs
  //  - id - filter by input ID
  //  - status.code - filter by input status
  Input input = 1;
  // RANK based predicted outputs from models such as custom trained models, pre-trained models,
  // etc. This is also where you enter the image url for a visual search because what we're asking
  // the system to do is find output embedding most visually similar to the provided input (that
  // input being in And.output.input.data.image.url for example). This will return the Hits
  // sorted by visual similarity (1.0 being very similar or exact match and 0.0 being very
  // dissimlar). For a search by Output concept, this means we're asking the system to rank
  // the Hits by confidence of our model's predicted Outputs. So for example if the model
  // predicts an image is 0.95 likely there is a "dog" present, that should related directly
  // to the score returned if you search for Output concept "dog" in your query. This provides
  // a natural ranking to search results based on confidence of predictions from the models and
  // is used when ANDing multiple of these types of RANK by Output queries together as well.
  //
  // ########## Supported fields ##########
  //  - data.clusters[].id
  //  - data.concepts[].id
  //  - data.concepts[].name
  //  - data.concepts[].value
  //  - input.data.image - empty image is required when searching by input ID
  //  - input.data.image.base64[]
  //  - input.data.image.url
  //  - input.id
  Output output = 2;
  // If True then this will flip the meaning of this part of the
  // query. This allow for queries such as dog AND ! metadata=={"blah":"value"}
  bool negate = 3;

  // FILTER by annotation information. This is more flexible than just filtering by
  // Input information because in the general case each input can have several annotations.
  // Some example use cases for filtering by annotations:
  // 1) find all the inputs annotated "dog" by worker_id = "XYZ"
  // 2) find all the annotations associated with embed_model_version_id = "123"
  // 3) find all the annotations that are trusted, etc.
  //
  // Since all the annotations under the hood are joined to the embedding model's annotation
  // using worker_id's of other models like cluster models or concept models should be
  // combinable with queries like visual search (a query with Output filled in).
  //
  // ########## Supported fields ##########
  //  - annotation_info - allows searching by empty annotation info
  //    note that searching by empty annotation info will actually not influence the search results.
  //    however, in order to be user-friendly, we are still supporting searching by empty annotation info.
  //  - annotation_info.fields - filter by annotation info
  //  - data.concepts[].id
  //  - data.concepts[].name
  //  - data.concepts[].value
  //  - data.geo.geo_box[].geo_point.latitude
  //  - data.geo.geo_box[].geo_point.longitude
  //  - data.geo.geo_limit.type
  //  - data.geo.geo_limit.value
  //  - data.geo.geo_point.latitude
  //  - data.geo.geo_point.longitude
  //  - data.image.url
  //  - data.metadata - allow search with empty metadata
  //    note that searching by empty metadata will actually not influence the search results.
  //    however, in order to be user-friendly, we are still supporting searching by empty metadata.
  //  - data.metadata.fields - filter by metadata. metadata key&value fields are OR-ed.
  //  - input_id
  //  - input_level
  //  - model_version_id
  //  - status.code
  //  - task_id
  //  - trusted
  //  - user_id
  Annotation annotation = 4;
}



// This is the search query used in /searches, model training requests, bulk data exports, etc.
message Query {
  // The query syntax is simply a list of And operatiosn that will be ANDed together to fetch
  // results which are returned to the user as Hit messages.
  //
  // Deprecated: Only used by the deprecated PostSearches endpoint. Use filters
  // and ranks instead with PostInputsSearches or PostAnnotationsSearches.
  repeated And ands = 1 [deprecated = true];

  // This allows the query to override any default language the app was setup in when doing Concept
  // based searches. This currently only affects public Models Output searches when those public
  // Models have translations for their Concepts.
  string language = 2;

  // filters in this query
  // e.q. only fetch annotations that have certain metadata
  repeated Filter filters = 3;

  // rankings in this query
  // e.g. visual search by a url
  repeated Rank ranks = 4;
}

// This is the new Search object used in saved searches.
message Search {
  // Search query.
  Query query = 1;

  // Customer facing, external ID for search to be saved. Provided by the user, e.g. "saved-search-1.
  // It is unique per application.
  string id = 2;

  // Application that owns this saved search.
  string application_id = 3;

  // Human readable display name of the saved search.
  string name = 4;

  // "As of" timestamp, indicating a time in the past as of which we want to
  // retrieve the annotations satisfying the query.
  google.protobuf.Timestamp as_of = 5;

  // Git hash of the code that ran the filter.
  string git_hash = 6;

  // When the saved search was created.
  google.protobuf.Timestamp created_at = 7;

  // When the saved search was updated.
  google.protobuf.Timestamp modified_at = 8;

  // The search algorithm to be used.
  // Options are are 'nearest_neighbor', 'brute_force', and 'avg_concept_brute_force'
  // The last two perform a brute force search visual search instead of a more scalable distributed
  // nearest neighbor search and should be used by advanced users only.
  // If not specified we default to nearest neighbor
  string algorithm = 9;

  // If true, save this search, and exit without executing the search.
  // If false execute the query
  bool save = 10;

  // Minimum value of confidence threshold score in result.
  // Defaults to 0.0 which means we won't do any thresholding as all probabilities will
  // likely be > 0.0.
  float min_value = 11;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 12;

  // Metric used for search. Can be EUCLIDEAN_DISTANCE (default) or COSINE_DISTANCE.
  // Currently only brute force search supports non-eudlicean metrics.
  enum Metric {
    METRIC_NOT_SET = 0;
    EUCLIDEAN_DISTANCE = 1;
    COSINE_DISTANCE = 2;
  }
  Metric metric = 13;
}

// Filter
message Filter {
  // If True then this will flip the meaning of this part of the
  // query. This allow for queries such as dog AND ! metadata=={"blah":"value"}
  bool negate = 3;

  // FILTER by annotation information.
  // ########## Supported fields ##########
  //  - annotation_info - allows searching by empty annotation info
  //    note that searching by empty annotation info will actually not influence the search results.
  //    however, in order to be user-friendly, we are still supporting searching by empty annotation info.
  //  - annotation_info.fields - filter by annotation info
  //  - data.clusters[].id
  //  - data.concepts[].id
  //  - data.concepts[].name
  //  - data.concepts[].value
  //  - data.frames[].frame_info - filter by frame annotations
  //  - data.geo.geo_box[].geo_point.latitude
  //  - data.geo.geo_box[].geo_point.longitude
  //  - data.geo.geo_limit.type
  //  - data.geo.geo_limit.value
  //  - data.geo.geo_point.latitude
  //  - data.geo.geo_point.longitude
  //  - data.metadata - allow search with empty metadata
  //    note that searching by empty metadata will actually not influence the search results.
  //    however, in order to be user-friendly, we are still supporting searching by empty metadata.
  //  - data.metadata.fields - filter by metadata. metadata key&value fields are OR-ed.
  //  - data.regions[].region_info.bounding_box - filter by bounding box annotations
  //  - data.regions[].region_info.mask - filter by mask annotations
  //  - data.regions[].region_info.point - filter by point annotations
  //  - data.regions[].region_info.polygon - filter by polygon annotations
  //  - data.regions[].region_info.span - filter by span annotations
  //  - data.text - filter by text annotations
  //  - data.time_segments[].time_info - filter by time-segment annotations
  //  - id
  //  - input_id
  //  - input_level
  //  - model_version_id
  //  - status.code
  //  - task_id
  //  - trusted
  //  - user_id
  //  - workflow_version_id
  Annotation annotation = 4;

  // FILTER by input information.
  // ########## Supported fields ##########
  //  - data.audio - filter audio inputs
  //  - data.image - filter image inputs
  //  - data.text - filter text inputs
  //  - data.video - filter video inputs
  //  - dataset_ids[] - filter by dataset IDs
  //  - status.code - filter by input status
  Input input = 5;

  // Filter by annotation last updated time range.
  TimeRange last_updated_time_range = 6;
}

// TimeRange
message TimeRange {
  google.protobuf.Timestamp start_time = 1; // Begin of the time range, optional, inclusive.
  google.protobuf.Timestamp end_time = 2;   // End of the time range, optional, inclusive.
}

// Rank
message Rank {
  // If True then this will flip the meaning of this part of the
  // query. This allow for queries such as !dog
  bool negate = 3;

  // RANK by annotation information.
  // ########## Supported fields ##########
  //  - data.concepts[].id
  //  - data.concepts[].name
  //  - data.concepts[].value
  //  - data.embeddings[].num_dimensions
  //  - data.embeddings[].vector[]
  //  - data.image.base64[]
  //  - data.image.url
  //  - data.text.raw
  //  - input_id
  //  - model_version_id
  Annotation annotation = 4;
}

// AnnotationSearchMetrics
message AnnotationSearchMetrics {
  // The ground truth we are evaluating against
  clarifai.api.Search ground_truth = 1;

  // The set we are evaluating
  clarifai.api.Search search_to_eval = 2;

  // The metric result
  EvalMetrics metrics = 3;

  // data is filled out with the concepts used for this evaluation
  Data data = 4;

  // active_concept_count is the number of concepts for this evaluation
  uint32 active_concept_count = 5;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 6;
}




// Text
message Text {
  // This is a raw text string.
  string raw = 1;
  // Url to a text file
  string url = 2;
  bool allow_duplicate_url = 3;
  // The hosted field lists original text hosted in Clarifai storage. This field is currently used
  // only in response.
  HostedURL hosted = 4;
  // text info
  TextInfo text_info = 5;
}

message TextInfo {
  // count of characters in text
  int32 char_count = 1;
  // text encoding
  string encoding = 2;
}






enum APIEventType {
  API_EVENT_TYPE_NOT_SET = 0;

  // On Prem event types
  ON_PREM_PREDICT = 1;
  ON_PREM_TRAIN = 2;
  ON_PREM_SEARCH = 3;

  // Platform event types
}

enum UsageIntervalType {
  // undef UsageIntervalType is so that the interval field can be forced to be included
  undef = 0;
  day = 1;
  month = 2;
  year = 3;
}


// User
message User {
  reserved 13, 14;

  string id = 1;

  string primary_email = 2 [deprecated = true];
  string first_name = 3;
  string last_name = 4;
  string company_name = 5;
  string job_title = 19;
  string job_role = 20;
  // This specifies user intent when registering on clarifai
  string intention = 24;
  string bill_type = 7 [deprecated = true];

  // When the user was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 6;
  google.protobuf.Timestamp date_gdpr_consent = 8 [deprecated = true];
  google.protobuf.Timestamp date_tos_consent = 9 [deprecated = true];
  google.protobuf.Timestamp date_marketing_consent = 10 [deprecated = true];
  google.protobuf.Timestamp date_pii_consent = 23 [deprecated = true];

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 11 [deprecated = true];
  repeated EmailAddress email_addresses = 12 [deprecated = true];

  bool two_factor_auth_enabled = 15 [deprecated = true];
  uint32 teams_count = 16 [deprecated = true];

  // Is starred by the requesting user (only showed on get/list requests)
  // Please use PostUserStars/DeleteUserStars endpoints to star/unstar an user
  bool is_starred = 21;
  // How many users have starred the user (only showed on get/list requests)
  // Computed value, not editable
  int32 star_count = 22;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 17;

  // This is all the personal information of a user. GetUser/ListUsers will not return this
  // information unless the caller has the UserAccounts_Get scope on their key or is the user
  // themselves.
  UserDetail user_detail = 18;
}

// This message holds the confidential information from the User object that we don't want to expose
// to other users. It will be accessible only from /users/{user_id}/account and with the User scopes.
message UserDetail {
  reserved 8;

  string primary_email = 1;
  string bill_type = 2;
  google.protobuf.Timestamp date_gdpr_consent = 3;
  google.protobuf.Timestamp date_tos_consent = 4;
  google.protobuf.Timestamp date_marketing_consent = 5;
  google.protobuf.Timestamp date_pii_consent = 13;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 6;
  repeated EmailAddress email_addresses = 7;
  bool two_factor_auth_enabled = 9;
  uint32 teams_count = 10;
  string country = 11;
  string state = 12;
}

// EmailAddress
message EmailAddress {
  string email = 1 [(clarifai.api.utils.cl_show_if_empty) = true];
  bool primary = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  bool verified = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
}



// Password
message Password {
  // unencrypted password string
  string plaintext = 1;
}



// PasswordViolations
message PasswordViolations {
  // when new password length is shorter than minimum length set
  bool minimum_length = 1;
  // when new password length is longer than maximum length set
  bool maximum_length = 2;
  // there is no upper case letter in the new password when there should be at least one
  bool upper_case_needed = 3;
  // there is no lower case letter in the new password when there should be at least one
  bool lower_case_needed = 4;
  // there is no numerics in the new password when there should be at least one
  bool numeric_needed = 5;
  // there is no special character in the new password when there should be at least one
  bool non_alphanumeric_needed = 6;
  // when one of the N most recent old password is reused, N is specified by password_reuse_epoch in db.password_policies
  bool password_reuse = 7;
  // when either user's first, middle or last name is used in the new password
  bool exclude_names = 8;
  // when first part of user's email (exact string or after removing special characters) is used in the new password
  bool exclude_email = 9;
  // when there are confusing letters in the new password, such as o (first character of 'omega') vs 0 (zero)
  bool no_confusing_letters = 10;
  // when there are simple password patterns used, such as 12345678 or aaaaaaa1
  bool no_simple_passwords = 11;
  // when there are common vocabs from the common vocab list used
  bool no_common_vocabs = 12;
  // when the current password is contained in the new password or vice versa
  bool no_overlap_with_old = 13;
  // when password has to be changed becauase it's too old
  bool password_lifespan = 14;
}

// Video
message Video {
  // This is a URL to a publicly accessible video file. The platform will download this file server
  // side and then process.
  string url = 1;
  // The base64 field is using video file bytes directly in the request.
  // NOTE: if you're sending a json request, then this MUST be base64 encoded before sending (hence
  // the name here).
  // When using our grpc clients, you DO NOT need to base64 encode
  // it yourself since the clients know how to do this for you automatically and will avoid the
  // base64 encoding if they send a binary request.
  bytes base64 = 2;
  bool allow_duplicate_url = 4;

  // URL of thumbnail image, which is currently frame at position of 1s. This field is currently
  // used only in response.
  // Deprecated in favour of thumbnail_hosted, which also contains alternate sizes of thumbnail
  string thumbnail_url = 5 [deprecated = true];
  // The hosted field lists original video hosted in Clarifai storage. This field is currently used
  // only in response.
  HostedURL hosted = 6;
  // The hosted field lists various sizes of the vide thumbnail hosted in Clarifai storage, with 'thumbnail' as the full size
  // This field is currently used only in response.
  HostedURL hosted_thumbnail = 8;
  // video info
  VideoInfo video_info = 7;
}

message VideoInfo {
  // width
  int32 width = 1;
  // height
  int32 height = 2;
  // Frames per second of the video.
  float fps = 3;
  // video format
  string video_format = 4;
  // video track bit rate
  int32 bit_rate = 5;
  // video frame count
  int32 frame_count = 6;
  // video duration in seconds
  float duration_seconds = 7;
}


// Workflow
message Workflow {
  // The workflows's unique id.
  string id = 1;
  // The app the workflow belongs to
  string app_id = 2;

  // When the workflow was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 3;

  // The list of nodes retrieved from latest workflow version.
  // Each node can specify an input node that it connects to in order to define the graph.
  repeated WorkflowNode nodes = 4;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 5;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 6;

  // The user the workflow belongs to
  string user_id = 7;

  // When the workflow was last modified
  google.protobuf.Timestamp modified_at = 8;

  // Info about the workflow version used to return the latest version when listing Workflows.
  WorkflowVersion version = 9;

  // Is starred by the requesting user (only showed on get/list requests)
  // Please use PostWorkflowStars/DeleteWorkflowStars endpoints to star/unstar a workflow
  bool is_starred = 10;
  // How many users have starred the workflow (only showed on get/list requests)
  // Computed value, not editable
  int32 star_count = 11;

  // Short description about this workflow
  string description = 12;

  // Notes for the workflow
  // This field should be used for in-depth notes and supports up to 64Kbs.
  string notes = 13;

  // Tags from use_cases category
  repeated string use_cases = 14 [(clarifai.api.utils.cl_show_if_empty) = true];

  // Tags for check consents
  repeated string check_consents = 15 [(clarifai.api.utils.cl_show_if_empty) = true];

  // bookmark info. When set, this workflow is a bookmarked workflow of this app.
  // Info in this field will allow you to find/access original workflow.
  BookmarkOrigin bookmark_origin = 16;
  // Representative image for this workflow
  Image image = 17;
}

// WorkflowVersion
message WorkflowVersion {
  // Id of this version.
  string id = 1;

  // Workflow id for this version.
  string workflow_id = 2;

  // When the version was created.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 3;

  // Most recent time when the version was updated.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp modified_at = 4;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 5;

  // The list of nodes that make up the workflow version. Each node can specify an input node
  // that it connects to in order to define the graph.
  repeated WorkflowNode nodes = 6;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 7;

  // The app the workflow version belongs to.
  string app_id = 8;
  // The user the workflow version belongs to.
  string user_id = 9;

  // Short description about this workflow version
  string description = 10;

  // License associated to this workflow version
  string license = 11;
}

// WorkflowNode
message WorkflowNode {
  // An identifier for this node in the graph. This is used when connecting NodeInputs
  // together.
  string id = 1;

  // The model that will do the processing at this node. We only vlidate the model.id and
  // model.model_version.id fields.
  Model model = 2;

  // Each WorkflowNode can connect to multiple input nodes so that we can handle multi-model data
  // and more complex workflow operations.
  repeated NodeInput node_inputs = 3;
  // suppress the output for workflow prediction
  bool suppress_output = 4;
  // Used to override the output_info.data and output_info.params of the model specified by the node.
  // Values for fields_map, message, and output_config are ignored.
  OutputInfo output_info_override = 5;
}

// NodeInput represents inputs to a node of the graph.
message NodeInput {
  // The id to a connected WorkflowNode which will be used as an input for current WorkflowNode.
  string node_id = 1;
}

// WorkflowResult
message WorkflowResult {
  string id = 1;
  clarifai.api.status.Status status = 2;
  // When the object was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 3;
  Model model = 4;
  Input input = 5;
  repeated Output outputs = 6;
  // Indicate if the output of this model is suppressed.
  bool suppress_output = 7;
}


// WorkflowState
message WorkflowState {
  // A unique ID for the workflow state.
  // To start saving a state in a PostWorkflowResults request set this ID to "init"
  // and it will return a newly generated unique state id that you can then pass in subsequent
  // PostWorkflowResults calls. These state expire after 5 minutes between calls.
  string id = 1;
}

// AppDuplication
message AppDuplication {
  // the id of app duplication
  string id = 1;

  // The ID of an existing app you want to copy data into.
  //
  // If not provided, then we will create a new application as the destination instead.
  // The various new_app_* fields can be used to set fields of this new application.
  string existing_app_id = 8;

  // The ID to use when creating a new application.
  // You cannot set this field when copying into an existing app, i.e., when existing_app_is is set.
  //
  // If not provided, then it will be generated automatically.
  string new_app_id = 2;

  // The name to use when creating a new application.
  // You cannot set this field when copying into an existing app, i.e., when existing_app_is is set.
  //
  // If not provided, then the ID of the new application is also used as the name.
  string new_app_name = 3;

  // The description to use when creating a new application.
  // You cannot set this field when copying into an existing app, i.e., when existing_app_is is set.
  //
  // If not provided, then the description of the source application is copied.
  string new_app_description = 10;

  // the status of app duplication
  clarifai.api.status.Status status = 4;
  // when is the app duplication triggered
  google.protobuf.Timestamp created_at = 5;
  // The last time when is the status got updated
  google.protobuf.Timestamp last_modified_at = 6;
  // Only copy resources depending on the filters
  AppDuplicationFilters filter = 7;

  // contains progress for each requested filter
  repeated AppCopyProgress progress = 9;
}

message AppCopyProgress {
  string field = 1;
  int32 value = 2;
}

// AppDuplicationFilters
message AppDuplicationFilters {
  // Copy inputs what what it depends on: input level annotation and concepts
  bool copy_inputs = 1;
  // Copy only concepts
  bool copy_concepts = 2;
  // Copy annotations and what it depends on: inputs and concepts
  bool copy_annotations = 3;
  // Copy models and what it depends on: concepts
  bool copy_models = 4;
  // Copy workflows and what it depends on: models and concepts
  bool copy_workflows = 5;
}

// LabelOrder
message LabelOrder {
  // id of the order
  string id = 1;

  // name of the order
  string name = 2;
  // status of the order.
  // pending (QA lead review the order),
  // in progress (labeling in progress),
  // ready for release (passed clarifai QA and client can review)
  // success (released)
  clarifai.api.status.Status status = 3;

  // if set to true, automatically release the labels once passed clarifai review.
  bool auto_release = 4;

  // allow input without any tag.
  bool allow_empty_tag = 5;

  // User desired estimation when the task should be done
  google.protobuf.Timestamp desired_fulfill_time = 6;

  // Clarifai estimation when the task should be done .
  google.protobuf.Timestamp estimate_fulfill_time = 7;

  // task for this label order
  Task task = 8;

  // When the label order was created.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 9;

  // Most recent time when the label order was updated.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp modified_at = 10;
}

// Task is the work that needs to be done for labeling the inputs in an app.
message Task {
  // Unique ID for the task.
  string id = 1;

  // When the task was created.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 2;

  // Most recent time when the task was updated.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp modified_at = 3;

  // Task type.
  TaskType type = 4;

  // Description of the task.
  string description = 5;

  // Worker details.
  TaskWorker worker = 6;

  // List of concept ids used in the work of this task.
  // DEPRECATED: Use task.concepts instead.
  repeated string concept_ids = 7 [deprecated = true];

  // List of inputs used in this task will be taken from this source.
  TaskInputSource input_source = 8;

  // For model predictions on video: Sample delay for video predicting (1 frame per N milliseconds)
  uint32 sample_ms = 9;

  // AI assistant details.
  TaskAIAssistant ai_assistant = 10;

  // Review details.
  TaskReview review = 11;

  // Status of this task.
  clarifai.api.status.Status status = 12;

  // Add a title for this task to quickly recognise it in a list of tasks.
  string name = 13;

  AiAssistParameters ai_assist_params = 14;

  enum TaskType {
    TYPE_NOT_SET = 0;

    // Concepts classification tasks annotate concepts for the overall image, frame of video or section of text.
    CONCEPTS_CLASSIFICATION = 1;
    // Bounding box detection tasks annotate rectangular bounding box regions around each concept in an image, frame of video or section of text.
    BOUNDING_BOX_DETECTION = 2;
    // Polygon detection tasks annotate free-form regions around concepts in an image, frame of video or section of text.
    POLYGON_DETECTION = 3;
  }

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  Visibility visibility = 15;

  // The app the task belongs to.
  string app_id = 16;
  // The user the task belongs to.
  string user_id = 17;

  // The label order the task belongs to.
  string label_order_id = 18;

  // Ignore Task.concept_ids field if Task.TaskConcept are supplied.
  repeated TaskConcept concepts = 19;

  // Specify whether existing Annotations within the same app that are generated by other auto annotation tasks
  // with the specified Concept from the selected Model or Workflow should deleted before executing the Task
  bool delete_previous_annotations = 20;
}

// AiAssistParameters
message AiAssistParameters {
  // Min and max threshold values for approving annotations by default based on prediction score
  float min_threshold = 1;
  float max_threshold = 2;
  // ids of concept relations. Used in AI assist workflow
  repeated string concept_relation_ids = 3;
}

// TaskWorker
message TaskWorker {
  // Worker strategy.
  TaskWorkerStrategy strategy = 1;

  // Who will work on this task.
  // DEPRECATED: Use users.id instead.
  repeated string user_ids = 2 [deprecated = true];

  // Users who will work on this task.
  // When the 'worker.users' field is additionally requested, then all user
  // info is filled for the workers. Otherwise, only the user 'id' is filled.
  repeated User users = 4;

  // Models that will work on this task. For Auto Annotation Tasks. Currently only supports 1 entry.
  repeated Model models = 5;

  // Workflows that will work on this task. For Auto Annotation Tasks. Currently only supports 1 entry.
  repeated Workflow workflows = 6;

  // Info based on the worker strategy,
  oneof strategy_info { TaskWorkerPartitionedStrategyInfo partitioned_strategy_info = 3; }

  enum TaskWorkerStrategy {
    reserved 1;

    WORKER_STRATEGY_NOT_SET = 0;

    // The inputs will be partitioned in several partitions.
    // Each worker will label one or more input partitions.
    PARTITIONED = 2;

    // Each worker will label all inputs from input source.
    FULL = 3;
  }
}

// TaskWorkerPartitionedStrategyInfo
message TaskWorkerPartitionedStrategyInfo {
  // Define how the partitioning should work.
  TaskWorkerPartitionedStrategy type = 1;

  // How many workers will label each input.
  int32 workers_per_input = 2;

  // In case of weighted partitioning, map user ids to weights.
  // Each labeler will be assigned work proportional to its own weight as compared to the sum of total weight.
  //
  // EXAMPLE:
  // If we have 3 workers, and weights = {1: 30, 2: 30, 3: 40},
  // then first worker will have assigned 30% of the work,
  // second worker will have assigned 30% of the work,
  // and third worker will have assigned 40% of the work.
  // You may use weights which add up to 100, but it's not necessary.
  // For example, weights {1: 30, 2: 30, 3: 40} are equivalent with {1: 3, 2: 3, 3: 4}
  // because they represent the same percentages: {1: 30%, 2: 30%, 3: 40%}.
  //
  // NOTE:
  // Note that no worker should be assigned a weight percentage greater than 1/workers_per_input.
  // It is mathematically impossible to partition the work in such a case.
  // Why? Say, we have 3 workers. And workers_per_input = 2, i.e. each input must be labeled by 2 workers.
  // Let's assign weights {1: 51%, 2: 25%, 3: 24%}.
  // Note that first worker has a weight percentage higher than 1/workers_per_input = 1/2 = 50%.
  // If we have 100 inputs, then a total of 100 * workers_per_input = 200 cumulative inputs will be labeled by these 3 workers.
  // Worker 1 should label 102 cumulative inputs, while worker 2 and worker 3 will label 98 cumulative inputs together.
  // No matter how we assign the 98 cumulative inputs, the 2 workers will be able to label up to 98 actual inputs.
  // This means the remaining 2 inputs will be labeled only by worker 1. This contradicts the worker_per_input = 2 requirement.
  google.protobuf.Struct weights = 3;

  enum TaskWorkerPartitionedStrategy {
    PARTITIONED_WORKER_STRATEGY_NOT_SET = 0;

    // Each worker will label (approximately) the same number of inputs.
    EVENLY = 1;

    // Each worker will have an assigned weight.
    // See weights field for more details.
    WEIGHTED = 2;
  }
}

// TaskInputSource
message TaskInputSource {
  // Type of input source.
  TaskInputSourceType type = 1;

  // If type is SAVED_SEARCH, then this is the saved search id.
  string id = 2;

  enum TaskInputSourceType {
    INPUT_SOURCE_TYPE_NOT_SET = 0;

    // Use all inputs in the app.
    ALL_INPUTS = 1;
    // Use the inputs from a saved search.
    SAVED_SEARCH = 2;
    // Inputs from a dataset.
    DATASET = 3;
  }
}

// TaskReview
message TaskReview {
  // Task review strategy.
  TaskReviewStrategy strategy = 1;

  // Who will review this task.
  // DEPRECATED: Use users.id instead.
  repeated string user_ids = 2 [deprecated = true];

  // Users who will review this task.
  // When the 'review.users' field is additionally requested, then all user
  // info is filled for the reviewers. Otherwise, only the user 'id' is filled.
  repeated User users = 5;

  // Info based on the review strategy,
  oneof strategy_info {
    TaskReviewManualStrategyInfo manual_strategy_info = 3;
    TaskReviewConsensusStrategyInfo consensus_strategy_info = 4;
  }

  enum TaskReviewStrategy {
    TASK_REVIEW_STRATEGY_NOT_SET = 0;

    // No review is needed.
    NONE = 1;

    // Manual review strategy.
    MANUAL = 2;

    // Consensus review strategy.
    CONSENSUS = 3;
  }
}

// TaskReviewManualStrategyInfo
message TaskReviewManualStrategyInfo {
  // This field represents the percentage of inputs that will be reviewed by reviewers. It is a value between 0 and 1.
  float sample_percentage = 1;
}

// TaskReviewConsensusStrategyInfo
message TaskReviewConsensusStrategyInfo {
  reserved 1;

  // The number of labelers that need to agree in order to automatically approve an annotation.
  uint32 approval_threshold = 2;
}

// TaskAIAssistant
message TaskAIAssistant {
  // The worker is helped by an AI assistant.
  // This field is the workflow id which is used to assist the worker with predictions.
  // If empty, then AI assistant is disabled.
  string workflow_id = 1;
}

// TaskStatusCountPerUser can represents count of human created annotations for a user for each valid status,
// count of inputs (anchor annotation) for a user for each valid status
message TaskStatusCountPerUser {
  string user_id = 1;
  uint32 pending = 2 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 awaiting_review = 3 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 success = 4 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 review_denied = 5 [(clarifai.api.utils.cl_show_if_empty) = true];
  uint32 awaiting_consensus_review = 6 [(clarifai.api.utils.cl_show_if_empty) = true];
}

message ThresholdRange {
  // The range used to filter over concept values.
  // e.g. GREATER_THAN_OR_EQUAL_TO 0.7 -> is_lower_inclusive = true, lower = 0.7, is_upper_inclusive = true, upper = 1.0
  // e.g. (0.3, 0.75] -> is_lower_inclusive = false, lower = 0.3, is_upper_inclusive = true, upper = 0.75
  bool is_lower_inclusive = 1;
  bool is_upper_inclusive = 2;
  float lower = 3;
  float upper = 4;
}

message TaskConceptAutoAnnotationConfig {
  // Filter anontations by their annotation data type.
  // This specifies types in an OR fashion, e.g. a `dog` concept that appears as a mask or a bbox.
  uint32 annotation_data_types = 1;

  // Filter annotations by concept value.
  // Only concepts that fit in the threshold will be used to generate annotations.
  ThresholdRange threshold_range = 2;
  // The output annotations will be created using this status code.
  clarifai.api.status.StatusCode status_code = 3;
}

message TaskConcept {
  // For auto annotation, id/name and value, user + app id must be specified. For other tasks, only the id field is required.
  Concept concept = 1;
  TaskConceptAutoAnnotationConfig auto_annotation_config = 2;
}
enum AnnotationDataType {
  ANNOTATION_DATA_TYPE_NOT_SET = 0;
  TAG = 1;
  BOUNDING_BOX = 2;
  POLYGON = 4;
  POINT = 8;
  SPAN = 16;
  MASK = 32;
}


enum RoleType {
  TEAM = 0;
  ORG = 1;
}









// Collector is a data pathway from a CollectorSource to an app to collect data automatically.
// For example, a CollectorSource
message Collector {
  // Unique ID for the collector.
  string id = 1;

  // Human readable description for the collector.
  string description = 2;

  // When the collector is created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 3;

  // This is a workflow to run inline in model predict calls. It should ONLY have very fast and
  // light-weight models in it as it will effect the speed of the predictions being made.
  // This workflow's purpose is to filter down the inputs to queue for the collector to process.
  // The input to this workflow is going to be the OUTPUT of the model, not the input to the model
  // since we want to encourage having fast workflows that can also take advantage of the model
  // outputs to make deciions (for example: thresholding based on concepts). If the workflow
  // output has any field that is non-empty then the input will be queued for the collector
  // to process with the post_queue_workflow_id.
  string pre_queue_workflow_id = 4;

  // A workflow to run to after the collector is processing the queued input. This workflow
  // uses the original input to the model as input to the workflow so that you can run additional
  // models as well on that input to decide whether to queue the model or not. If the workflow
  // output has any field that is non-empty then it will be passed on to POST /inputs to
  // the destination app.
  string post_queue_workflow_id = 5;

  // The source of the collector to feed data into this app.
  // Note(zeiler): if we wanted more than one source per collector we could make this it's own
  // object and introduce /collectors/{collector_id}/sources
  // We will keep it simple for now and have just one source per collector since a user can make
  // more than one collector in the same app anyways.
  CollectorSource collector_source = 6;

  // This is the workflow ID to do POST /inputs with the collected data using.
  // This needs to be present at all times in this app for the collector to work.
  // If this is not specified then it will use the default_workflow_id of the app.
  // Note(zeiler): not yet available, uses only the default workflow that POST /inputs uses.
  // string workflow_id = 7;

  // Status for the collector. This allows you to pause a collector without having to delete it as
  // an example.
  clarifai.api.status.Status status = 7;
}

// Configuration for the source to collect data from.
// Only one of the fields can be present at a time.
message CollectorSource {
  // The ID of the source in case we want to implment /collectors/{collector_id}/sources
  // string id = 1;

  // Collect from the inputs passed in for PostModelOutputs predictions of a specific model.
  // This does not apply to models used within workflows, only PostModelOutputs calls.
  APIPostModelOutputsCollectorSource api_post_model_outputs_collector_source = 2;
}

// This is configuration for using the inputs send for model prediction in our API as
// as the source for data.
message APIPostModelOutputsCollectorSource {
  // To define the model that we should collect from we need to specify the following 4 IDs:
  // The User ID of the model we want to collect from.
  // This is User B in the example.
  string model_user_id = 1;
  // The App ID of the model we want to collect from.
  string model_app_id = 2;
  // The Model ID of the model we want to collect from.
  string model_id = 3;
  // The Version ID of the model we want to collect from.
  string model_version_id = 4;

  // This key is used to POST /inputs into your app by the collector. It can be an API key or a
  // PAT. This needs the permissions that are needed for POST /inputs for the app_id this
  // Collector is defined in.
  string post_inputs_key_id = 5;

  // The most flexible scenario is User C creates a collector and she wants to ingest User A's
  // predictions of User B's model into their app (User C's app), for which User C has created
  // the annotation workflow using a combination of models, perhaps from User D even.

  // The User ID of the caller of the model we want to collect from.
  // This is needed because the below Model's ids could be used by multiple users like the
  // clarifai/main models are or any model that has been shared with a collaborator. Therefore we
  // need to know which caller of the model to collect inputs from.
  // This is User A in the example.

  // This is a private field that defaults to the app owner for public users.
  // If this is left blank then this collector will collect from ALL users calling the given model.
  string caller_user_id = 6;
}

// StatValue
message StatValue {
  // The time of the event. Defaults to now().
  google.protobuf.Timestamp time = 1;

  // A value for the metric you're recording.
  float value = 2;

  // List of tags to attach to this stat. Each should contain one colon so that the first part will
  // be used as a tag group while the second being the tag itself. For example: ["task_id:a",
  // "worker_id:1"]. These tag groups like "task_id" or "worker_id" are important for aggregating
  // values in the StatValueAggregateQuery.
  repeated string tags = 3;
}

// StatValueAggregateResult
message StatValueAggregateResult {
  // The list of repeated aggregate values and their counts.
  repeated StatValueAggregate stat_value_aggregates = 1;

  // The query that created these results.
  StatValueAggregateQuery stat_value_aggregate_query = 2;
}

// StatValueAggregate
message StatValueAggregate {
  // The time of the aggregation. For example, if you aggregate over "HOUR" buckets then you can
  // expect each hour that has atleast one value (matching the rest of your query fields) will have
  // a StatValueAggregate with the time filled into that hour.
  google.protobuf.Timestamp time = 1;
  // The value aggregated according to the stat_value_agg_type
  float aggregate_value = 2;
  // The count of the stat values that were used in this aggregation.
  uint64 count = 3;
  // The tags for this aggregated_value and count. This will be filled in if tag groups were used in
  // the query to group aggregations.
  repeated string tags = 4;
}

// StatValueAggregateQuery
message StatValueAggregateQuery {
  // These tags are used to filter down the values before they are aggregated. For example,
  // if you want to aggregate values for "task_id:a" you could specify that as a tag here.
  repeated string tags = 1;

  // These are tag groups to aggregate over. So for example if you added stat values with tags
  // "task_id:a" and others with "task_id:b", then added ["task_id"] to the task group, it the
  // aggregation would return StatValueAggregate values for each task_id. If you provide more than
  // one tag_group the response will return all rolled up combinations of them. For example
  // ["task_id", "something"] where "something:1" and "something:2" were used as tags for some
  // values then you'd get StatValueAggregate values back for:
  // task_id | something
  // a       | 1
  // a       | 2
  // b       | 1
  // b       | 1
  repeated string tag_groups = 2;

  // Aggregation function to use over the values. Count(value) is also always returns.
  // Defaults to 'sum' if not provided.
  StatValueAggType stat_value_agg_type = 3;

  // Aggregation bins for time where the values will be aggregated at this bin granualarity.
  // And the "time" field will be returned in StatValueAggregate object.
  // If not provided then bins are not used, and all time is aggregated over.
  StatTimeAggType stat_time_agg_type = 4;

  // If provided the time range over which values will be >= this time. If not provided then
  // all values will be used back to start of time.
  google.protobuf.Timestamp start_time = 5;

  // If provided the time range over which values will be <= this time. If not provided then all
  // values will be used up until now().
  google.protobuf.Timestamp end_time = 6;
}
enum StatValueAggType {
  SUM = 0;
  AVG = 1;
}

enum StatTimeAggType {
  NO_TIME_AGG = 0;
  YEAR = 1;
  MONTH = 2;
  WEEK = 3;
  DAY = 4;
  HOUR = 5;
  MINUTE = 6;
}













message DatasetInputsSearchAddJob {
  // The id of this job
  string id = 1;

  // When the job was created.
  google.protobuf.Timestamp created_at = 2;

  // When the job was last modified.
  google.protobuf.Timestamp modified_at = 3;

  // Status of the job and rough estimated progress
  clarifai.api.status.Status status = 4;

  // Dataset which will receive inputs
  string dataset_id = 5;

  // The search that the job uses
  Search search = 6;
}

// PCAProjectionComparator
message PCAProjectionComparator {
  // Within what distance do we consider two annotations duplicates
  float distance_threshold = 1;
  // What cluster model version generated these
  string model_version_id = 2;
}

// DuplicateAnnotationsResults
message DuplicateAnnotationsResults {
  repeated string duplicate_cfid = 1;
  int32 unique_count = 2;
}

// Visibility represents how visible the given resource is to other users.
// When authenticating a request we can tell if a user is a collaborator or a teammate for the
// the app that contains the resource and set their allowed visibility. We use that to restrict
// what they are allowed to see:
// If AllowedVisibility is PRIVATE then we allow PRIVATE (10), ORG (30), PUBLIC (50)
// If AllowedVisibility is ORG then we allow ORG (30), PUBLIC (50)
// If AllowedVisibility is PUBLIC then we allow PUBLIC (50) only.
message Visibility {
  // Gettable defined the level of access for GET operations for this resource.
  enum Gettable {
    // Default value not allowed.
    UNKNOWN_VISIBILITY = 0;
    // PRIVATE requires collaborator or team permissions in order to GET this resource.
    PRIVATE = 10;
    // ORG requires you to be in the same org in order to GET this resource, but don't have to be a
    // teammate or collaborator.
    ORG = 30;
    // PUBLIC opens up GET access to the resource to any user on the platform even if they are not
    // a teammate or collaborator.
    PUBLIC = 50;
  }
  Gettable gettable = 1;
}

// TrendingMetric
message TrendingMetric {
  string user_id = 1;
  string app_id = 2;
  string object_id = 3;
  uint64 view_count = 4;
}

enum ValidationErrorType {
  VALIDATION_ERROR_TYPE_NOT_SET = 0;

  RESTRICTED = 1;
  DATABASE = 2;
  FORMAT = 3;
}





message FullTag {
  // Display name of the tag. Ex. "English"
  string name = 1;
  // Id value for referencing. Ex. "en"
  string id = 2;
}

// TimeSegment
message TimeSegment {
  // A unique id for the time segment.
  string id = 1;

  Data data = 2;

  TimeInfo time_info = 3;
}

// TimeInfo
message TimeInfo {
  reserved 2,3;
  // Number of frames
  uint32 num_frames = 1;
  // Timestamp where track begins.
  float begin_time = 4;
  // Timestamp where track ends.
  float end_time = 5;
}





// DatasetStar
message DatasetStar { string dataset_id = 1; }

// ModuleStar
message ModuleStar {
  // Module id of the star
  string module_id = 1;
}



// An app module that a user created in our app module marketplace.
message Module {
  reserved 2;
  // A unique ID for this app module.
  string id = 1;
  // A short description for this app module to be used in grids of modules.
  string description = 3;
  // When the app module was created.
  google.protobuf.Timestamp created_at = 4;
  // When the app module was last modified.
  google.protobuf.Timestamp modified_at = 5;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  // Defaults to PRIVATE if not provided.
  Visibility visibility = 7;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  // This is an optional arg.
  google.protobuf.Struct metadata = 8;

  // The creator of the app module.
  string user_id = 9;

  // The app_id this module was created in.
  string app_id = 10;

  // A ModuleVersion which is used when listing modules to include the latest module version
  // in the response.
  ModuleVersion module_version = 11;

  // Is starred by the requesting user (only showed on get/list requests)
  // Please use PostModuleStars/DeleteModuleStars endpoints to star/unstar a module
  bool is_starred = 12;
  // How many users have starred the module (only showed on get/list requests)
  // Computed value, not editable
  int32 star_count = 13;

  // bookmark info. When set, this module is a bookmarked module of this app.
  // Info in this field will allow you to find/access original module.
  BookmarkOrigin bookmark_origin = 14;
  // Representative image for this module
  Image image = 15;
}

// A specific version of an app module that is available for assigning to apps.
message ModuleVersion {
  reserved 5;
  // A name for this version like 1_0, 1_1_0, etc.
  string id = 1;
  // The module this version belongs to.
  string module_id = 2;
  // The app_id this module version belongs to.
  string app_id = 3;
  // The user_id this module version belongs to.
  string user_id = 4;
  // A short description for this version.
  string description = 6;
  // A markdown formatted string to detailed description of the app module.
  // This is within each version so that it can be change version to version.
  string notes = 7;
  // When the app module version was created.
  google.protobuf.Timestamp created_at = 8;
  // When the app module version was last modified.
  google.protobuf.Timestamp modified_at = 9;

  // The code repo of the streamlit app.
  // If you are still developing your Module you should create a ModuleVersion
  // with an empty git_commit_url and then create an InstalledModuleVersion
  // with a pre-deployed deploy_url (such as localhost or streamlit cloud).
  // Once you are ready to create a production, create a new ModuleVersion with
  // the ready git url to a specific commit that you would like to be reviewed by the
  // Clarifai team for approval within our community. You cannot publish a ModuleVersion
  // is reviewed and approved. Please only provide the git_commit_url when you're
  // ready for a review. This url needs to include a specific commit, for example:
  // https://github.com/user/repo/commit/767ff9c08ba3429c8e7b8825da148555
  string git_commit_url = 10;

  message ModuleSubNav {
    // This is the display title for a navbar element to link to a specific page.
    // The name for this subnav element to show in the sidebar.
    string title = 1;
    // The query param name
    string query_key = 2;
    // The query param value
    string query_value = 3;
  }

  message ModuleNav {
    // This is the left side title for this module and for browser tab title of the module.
    // We have this in the version so that users can change those settings
    // when releasing a new version of their module.
    string title = 1;

    // A list of subnav elements to put under the module title.
    repeated ModuleSubNav module_sub_navs = 2;
  }
  ModuleNav module_nav = 11;

  // A boolean to mark if Clarifai has approved this app version.
  // This cannot be set in the request to True.
  bool approved = 12;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible.
  // Defaults to PRIVATE if not provided.
  Visibility visibility = 13;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  // This is an optional arg.
  google.protobuf.Struct metadata = 14;
}

message InstalledModuleVersion {
  // A unique id for this install. This will be used in the browser url.
  string id = 1;
  // The installed module version provided here so that we users don't need to do an additional
  // fetch. When creating a new InstalledModuleVersion you should provide the:
  // module_version.user_id
  // module_version.app_id
  // module_version.module_id
  // module_version.id
  // in order to uniquely define which module version.
  ModuleVersion module_version = 2;
  // The app_id the ModuleVersion is installed into (not necessary where the ModuleVersion was
  // created). This doesn't have to be provided in requests to install, but will be returned in
  // responses.
  string app_id = 3;
  // The user that the app belongs to where the ModuleVersion is installed into (not necessary where
  // the ModuleVersion was created). This doesn't have to be provided in requests to install, but
  // will be returned in responses.
  string user_id = 4;
  // When the install was created.
  google.protobuf.Timestamp created_at = 5;
  // When the install was last modified.
  google.protobuf.Timestamp modified_at = 6;

  // The URL of where this app module version is deployed.
  // If you provide this deploy_url when creating the install then it will
  // be treated as a pre-deployed module. You can only use a pre-deployed module
  // in when installing to an app_id that you own as the creator of the module.
  // If you want to install someone elses module or to rely on Clarifai deploying
  // your module for you, leave deploy_url empty when creating the install.
  // If it is left empty, then deployment will occur when this module version is
  // installed into an app using the git_commit_url of the ModuleVersion.
  string deploy_url = 7;

  // The visibility field represents whether this message is privately/publicly visible.
  // To be visible to the public the App that contains it AND the User that contains the App must
  // also be publicly visible. For the InstalledModuleVersion this allows the app owner who
  // installed the module version to decide if they want other users of their app to have
  // the added functionality that the modules version provides to their app.
  // Defaults to PRIVATE if not provided.
  Visibility visibility = 8;

  // The key ID to use for making requests to the API for this module.
  // This key is associated to this installed module version by PostInstalledModuleVersionsKey
  // request. The key is associated with the CALLER not the App Owner where this module is installed
  // nor the author of the module. This allows the module to act on behalf of the caller at all
  // times so we get proper permissions the caller has (such as if they are stranger, teammate or
  // collaborator). This key should be a personal access token to enable modules to work across apps
  // and have necessary abilities beyond what app-specific keys offer.
  string key_id = 9;
}

message BulkOperation {
  // id of the Bulk Operation task
  string id = 1;

  // Input Source could be list of input ids or a Search whose results will be a list of input ids.
  // InputIDs:
  //      List of input ids to which operation to be applied
  // clarifai.api.Search:
  //      A Search(either filter or rank with min value) to allow filtering down the entire app's
  //      sub-assets(image, region in image, frame in video, region in frame in video)
  //      and perform operation to only the results of this search query. See our search
  //      documentation for more details about the search Query message.
  //      For eg., filters the asset/sub-asset matching the search and performs specified operation.
  // Dataset:
  //      A dataset, whose inputs will have the operation applied to. This does not support dataset versions.
  oneof input_source {
    InputIDs input_ids = 2;
    clarifai.api.Search search = 10;
    Dataset dataset = 11;
  }

  // Operation to perform
  Operation operation = 3;

  // Application ID that this Operation was created from
  string app_id = 4;

  // Status (pending, in-progress, completed, failed) of the operation
  clarifai.api.status.Status status = 5;

  // Progress of an on-going Bulk Operation task
  Progress progress = 6;

  // User id that created this operation
  string created_by = 7;

  // When the operation was created. We follow the XXXX timestamp
  // format. We use https://www.ietf.org/rfc/rfc3339.txt format:
  // "2006-01-02T15:04:05.999999Z" so you can expect results like
  // the following from the API:
  // "2017-04-11T21:50:50.223962Z"
  google.protobuf.Timestamp created_at = 8;
  // Last time the status got updated
  google.protobuf.Timestamp last_modified_at = 9;
}

message InputIDs { repeated string input_ids = 1; }

message Progress {
  uint32 processed = 1;
  string last_processed_id = 2;
}

message Operation {
  // Bulk Operations supported:
  // Concepts:
  //    Operations: add_concepts, delete_concepts
  //    AddConcepts:
  //        If new concepts are given, add concepts operation creates new concepts in the app and adds them to the given inputs' annotations.
  //        If the given concept already exist, the label value of the concept is updated with the given value.
  //    DeleteConcepts:
  //        Remove the matching concept(s) for all the inputs in input source (mentioned above).
  //        If user IDs are set, concepts will be deleted only from annotations created by given user ids.
  //        If the user IDs are not set, the list will be automatically set with 1 element that is the caller user ID.
  //    Input Source:
  //        Input ids of assets(images) (or) search on sub-assets(region in image, frame in video, region in frame in video)
  // Metadata:
  //    Operations: add_metadata, delete_metadata
  //    AddMetadata:
  //        Add the provided metadata to the input level annotation for all the inputs in input source (mentioned above).
  //        If the key(s) already exists, it will overwrite the key(s) with the corresponding new value(s).
  //    DeleteMetadata:
  //        Remove the key, value pairs that match the given metadata from the existing input level Annotations' metadata
  //        for all the inputs in input source (mentioned above).
  //    Input Source:
  //        Input ids of assets(images, videos) (or) search on sub-assets(region in image, frame in video, region in frame in video)
  // Geo:
  //    Operations: overwrite_geo, delete_geo
  //    OverwriteGeo:
  //        Add the provided geo info for all the inputs in input source (mentioned above).
  //    DeleteGeo:
  //        Delete Geo info for all the inputs in input source (mentioned above).
  //    Input Source:
  //        Input ids of assets(images, videos) (or) search on sub-assets(region in image, frame in video, region in frame in video)
  // Dataset Inputs:
  //    Operations: add_to_dataset, delete_from_dataset, split_into_datasets
  //    AddToDataset:
  //        Add inputs to a dataset
  //    DeleteFromDataset:
  //        Delete inputs from a dataset
  //    SplitIntoDatasets:
  //        Randomly split inputs into provided dataset ID's with provided percentages.
  oneof operation {
    AddConcepts add_concepts = 1;
    DeleteConcepts delete_concepts = 2;
    AddMetadata add_metadata = 3;
    DeleteMetadata delete_metadata = 4;
    OverwriteGeo overwrite_geo = 5;
    DeleteGeo delete_geo = 6;
    DeleteFromDataset delete_from_dataset = 7;
    AddToDataset add_to_dataset = 8;
    SplitIntoDatasets split_into_datasets = 9;
  }
}

message AddConcepts { repeated Concept concepts = 1; }

message DeleteConcepts {
  repeated Concept concepts = 1;
  repeated string user_ids = 2;
}

message AddMetadata {
  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 1;
}

message DeleteMetadata {
  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  google.protobuf.Struct metadata = 1;
}

message OverwriteGeo {
  // Geo info
  Geo geo = 1;
}

message DeleteGeo {}

message AddToDataset { string dataset_id = 1; }

message DeleteFromDataset { string dataset_id = 1; }

message SplitIntoDatasets {
  repeated DatasetSplit dataset_splits = 1;
  DatasetSplitMethod method = 2;
  enum DatasetSplitMethod {
    NOT_SET = 0;
    // We will randomly split inputs into the datasets
    RANDOM_PERCENTAGE_SPLIT = 1;
  }
}

message DatasetSplit {
  // Expected to have ID
  Dataset dataset = 1;
  oneof method_info {
    // For RANDOM_PERCENTAGE_SPLIT.
    // Values from (0,100]
    uint32 percentage = 2;
  }
}


message InputsAddJob {
  reserved 2, 5, 6;

  // id of the job
  string id = 1;

  // If call back url is set, we will send a Post request to this endpoint with job status.
  string call_back_url = 3;

  // Personal Access Token to the application to which inputs are added
  string app_pat = 4;

  // Progress of an on-going Input Ingestion task
  InputsAddJobProgress progress = 7;

  // When the job was created.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 8;

  // Most recent time when the job was updated.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp modified_at = 9;

  // Sub-jobs that extract inputs from the cloud and/or archives
  repeated InputsExtractionJob extraction_jobs = 10;

  // Archive uploads
  repeated Upload uploads = 11;

  // Status of the job
  clarifai.api.status.Status status = 12;
}

message InputsAddJobProgress {
  uint64 pending_count = 1;
  uint64 in_progress_count = 2;
  uint64 success_count = 3;
  uint64 failed_count = 4;
}

message Upload {
  // ID of upload
  string id = 1;

  // When the upload was started.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 2;

  // Most recent time when the upload was updated.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp modified_at = 3;

  // When the upload will expire and be deleted
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp expires_at = 4;

  // Status of the upload
  clarifai.api.status.Status status = 5;

  // name of uploaded content (e.g. filename)
  string content_name = 8;

  // Total size of the upload content
  uint64 content_length = 6;

  // Url of uploaded content
  string content_url = 7;
}

message UploadContentPart {
  uint64 range_start = 1;
  int64 part_number = 2;
  bytes data = 3;
}

// We use this message to communicate with Custom Code Operator Backend. You can use this message
// to test your custom operator code locally.
// We expect the response to come in the format of the MultiOutputResponse protobuf message.
message CustomCodeOperatorRequest {
  repeated Input inputs = 1;

  // support arbitrary metadata
  google.protobuf.Struct metadata = 1002;
}

message InputsExtractionJob {
  clarifai.api.status.Status status = 1;

  // ID of extraction job
  string id = 2;

  // Url of archive or bucket
  string url = 3;

  // Progress counts of the job
  InputsExtractionJobProgress progress = 4;

  // When the extraction job was started.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp created_at = 5;

  // Most recent time when the extraction job was updated.
  // The format is https://www.ietf.org/rfc/rfc3339.txt.
  // Example: "2006-01-02T15:04:05.999999Z".
  google.protobuf.Timestamp modified_at = 6;

  // How to handle input ID conflicts.
  InputIDConflictResolution input_id_conflict_resolution = 7;

  // Fields set in the template are added to all generated inputs
  Input input_template = 8;
}

message InputsExtractionJobProgress {
  reserved 1;

  uint64 audio_inputs_count = 2;
  uint64 image_inputs_count = 3;
  uint64 video_inputs_count = 4;
  uint64 text_inputs_count = 5;

  uint64 pending_archives_count = 6;
  uint64 in_progress_archives_count = 7;
  uint64 completed_archives_count = 8;
  uint64 failed_archives_count = 9;
}

message InputsDataSource {
  // Collect statistics about created inputs in job with given ID.
  // On Post call:
  // * If job ID is empty, then job is automatically created with random ID.
  // * If job ID is non-empty, then a new job will be created with given ID.
  string inputs_add_job_id = 1;

  DataSourceURL url = 2;

  // How to handle input ID conflicts.
  InputIDConflictResolution input_id_conflict_resolution = 3;

  // Fields set in the template will also be added to all generated inputs
  Input input_template = 4;
}

message DataSourceURL {
  // Supported providers are AWS S3, Azure blob, GCP cloud storage.
  string url = 1;

  // Credentials that would allow access to the provided url
  DataSourceCredentials credentials = 2;
}

message DataSourceCredentials {
  reserved 3;
  oneof credentials {
    // AWS S3 credentials for authentication.
    AWSCreds s3_creds = 1;
    // GCP Cloud Storage uses service account key data(creds.json) as Byte array for authentication.
    bytes gcp_creds = 2;
    // Azure Blob credentials for authentication.
    AzureBlobCreds azure_blob_creds = 4;
  }
}

// AWS S3 storage credentials.
message AWSCreds {
  reserved 1;

  string region = 2;
  string id = 3;
  string secret = 4;
  string token = 5;
}

// Azure Blob storage credentials.
message AzureBlobCreds {
  string account_name = 1;
  string account_key = 2;
}

message InputsUpload {
  // Collect statistics about created inputs in job with given ID.
  // * If job ID is empty, then job is automatically created with random ID.
  // * If job ID is non-empty, then a new job will be created with given ID.
  string inputs_add_job_id = 1;

  // Personal Access Token to the application to which inputs are added
  string app_pat = 2;

  Upload upload = 3;

  // How to handle input ID conflicts.
  InputIDConflictResolution input_id_conflict_resolution = 4;

  // Fields set in the template will also be added to all generated inputs
  Input input_template = 5;
}
enum InputIDConflictResolution {
  INPUT_ID_CONFLICT_RESOLUTION_NOT_SET = 0; // Defaults to SKIP

  SKIP = 1;   // Mark duplicate inputs as error and skip processing them.
  SUFFIX = 2; // Add a suffix to inputs with conflicting IDs. Attempts numeric suffixes "-1" to "-9" and then a randomized suffix. Identical ID's in the same request are still treated as errors.
}


message BookmarkOrigin {
  // original resource id
  string id = 1;

  // original resource app id
  string app_id = 2;

  // original resource user id
  string user_id = 3;

  enum BookmarkType {
    unknown = 0;
    model = 1;
    workflow = 2;
    dataset = 3;
    module = 4;
  }
  // resource type.
  BookmarkType resource_type = 4;
}

// An app module that a user created in our app module marketplace.
message Runner {
  // A unique ID for this app module.
  string id = 1;
  // A short description for this app module to be used in grids of modules.
  string description = 2;
  // When the app module was created.
  google.protobuf.Timestamp created_at = 3;
  // When the app module was last modified.
  google.protobuf.Timestamp modified_at = 4;

  // To handle arbitrary json metadata you can use a struct field:
  // https://github.com/google/protobuf/blob/master/src/google/protobuf/struct.proto
  // This is an optional arg.
  google.protobuf.Struct metadata = 5;

  // The creator of the app module.
  string user_id = 6;

  // Labels to match.
  repeated string labels = 7;
}